<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                Robotics-Related Articles.
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-21T00:00:00Z">2024-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">54</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras
  Based on Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianye Ding, Hongyu Li, Huaizu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obstacle detection and tracking represent a critical component in robot
autonomous navigation. In this paper, we propose ODTFormer, a Transformer-based
model to address both obstacle detection and tracking problems. For the
detection task, our approach leverages deformable attention to construct a 3D
cost volume, which is decoded progressively in the form of voxel occupancy
grids. We further track the obstacles by matching the voxels between
consecutive frames. The entire model can be optimized in an end-to-end manner.
Through extensive experiments on DrivingStereo and KITTI benchmarks, our model
achieves state-of-the-art performance in the obstacle detection task. We also
report comparable accuracy to state-of-the-art obstacle tracking models while
requiring only a fraction of their computation cost, typically ten-fold to
twenty-fold less. The code and model weights will be publicly released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDP Synthesis of Maximum Coverage Trees for Probabilistic Planning under
  <span class="highlight-title">Control</span> Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naman Aggarwal, Jonathan P. How
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents Maximal Covariance Backward Reachable Trees (MAXCOVAR
BRT), which is a multi-query algorithm for planning of dynamic systems under
stochastic motion uncertainty and constraints on the control input with
explicit coverage guarantees. In contrast to existing roadmap-based
probabilistic planning methods that sample belief nodes randomly and draw edges
between them \cite{csbrm_tro2024}, under control constraints, the reachability
of belief nodes needs to be explicitly established and is determined by
checking the feasibility of a non-convex program. Moreover, there is no
explicit consideration of coverage of the roadmap while adding nodes and edges
during the construction procedure for the existing methods. Our contribution is
a novel optimization formulation to add nodes and construct the corresponding
edge controllers such that the generated roadmap results in provably maximal
coverage under control constraints as compared to any other method of adding
nodes and edges. We characterize formally the notion of coverage of a roadmap
in this stochastic domain via introduction of the h-$\operatorname{BRS}$
(Backward Reachable Set of Distributions) of a tree of distributions under
control constraints, and also support our method with extensive simulations on
a 6 DoF model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extended Reality for Enhanced Human-Robot Collaboration: a
  Human-in-the-Loop Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehor Karpichev, Todd Charter, Homayoun Najjaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of automation has provided an opportunity to achieve higher
efficiency in manufacturing processes, yet it often compromises the flexibility
required to promptly respond to evolving market needs and meet the demand for
customization. Human-robot collaboration attempts to tackle these challenges by
combining the strength and precision of machines with human ingenuity and
perceptual understanding. In this paper, we conceptualize and propose an
implementation framework for an autonomous, machine learning-based manipulator
that incorporates human-in-the-loop principles and leverages Extended Reality
(XR) to facilitate intuitive communication and programming between humans and
robots. Furthermore, the conceptual framework foresees human involvement
directly in the robot learning process, resulting in higher adaptability and
task generalization. The paper highlights key technologies enabling the
proposed framework, emphasizing the importance of developing the digital
ecosystem as a whole. Additionally, we review the existent implementation
approaches of XR in human-robot collaboration, showcasing diverse perspectives
and methodologies. The challenges and future outlooks are discussed, delving
into the major obstacles and potential research avenues of XR for more natural
human-robot interaction and integration in the industrial landscape.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Jin Li, Mariia Gladkova, Yan Xia, Rui Wang, Daniel Cremers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works on the global place recognition treat the task as a retrieval
problem, where an off-the-shelf global descriptor is commonly designed in
image-based and LiDAR-based modalities. However, it is non-trivial to perform
accurate image-LiDAR global place recognition since extracting consistent and
robust global descriptors from different domains (2D images and 3D point
clouds) is challenging. To address this issue, we propose a novel
Voxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel
correspondences in a self-supervised manner and brings them into a shared
feature space. Specifically, VXP is trained in a two-stage manner that first
explicitly exploits local feature correspondences and enforces similarity of
global descriptors. Extensive experiments on the three benchmarks (Oxford
RobotCar, ViViD++ and KITTI) demonstrate our method surpasses the
state-of-the-art cross-modal retrieval by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page https://yunjinli.github.io/projects-vxp/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-<span class="highlight-title">Optimization</span> of Environment and Policies for Decentralized
  Multi-Agent <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Gao, Guang Yang, Amanda Prorok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work views the multi-agent system and its surrounding environment as a
co-evolving system, where the behavior of one affects the other. The goal is to
take both agent actions and environment configurations as decision variables,
and optimize these two components in a coordinated manner to improve some
measure of interest. Towards this end, we consider the problem of decentralized
multi-agent navigation in cluttered environments. By introducing two
sub-objectives of multi-agent navigation and environment optimization, we
propose an $\textit{agent-environment co-optimization}$ problem and develop a
$\textit{coordinated algorithm}$ that alternates between these sub-objectives
to search for an optimal synthesis of agent actions and obstacle configurations
in the environment; ultimately, improving the navigation performance. Due to
the challenge of explicitly modeling the relation between agents, environment
and performance, we leverage policy gradient to formulate a model-free learning
mechanism within the coordinated framework. A formal convergence analysis shows
that our coordinated algorithm tracks the local minimum trajectory of an
associated time-varying non-convex optimization problem. Extensive numerical
results corroborate theoretical findings and show the benefits of
co-optimization over baselines. Interestingly, the results also indicate that
optimized environment configurations are able to offer structural guidance that
is key to de-conflicting agents in motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Hierarchical <span class="highlight-title">Control</span> For Constrained Dynamic Task Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlott Vallon, Alessandro Pinto, Bartolomeo Stellato, Francesco Borrelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel data-driven hierarchical control scheme for
managing a fleet of nonlinear, capacity-constrained autonomous agents in an
iterative environment. We propose a control framework consisting of a
high-level dynamic task assignment and routing layer and low-level motion
planning and tracking layer. Each layer of the control hierarchy uses a
data-driven MPC policy, maintaining bounded computational complexity at each
calculation of a new task assignment or actuation input. We utilize collected
data to iteratively refine estimates of agent capacity usage, and update MPC
policy parameters accordingly. Our approach leverages tools from iterative
learning control to integrate learning at both levels of the hierarchy, and
coordinates learning between levels in order to maintain closed-loop
feasibility and performance improvement of the connected architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion
  Descriptors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Tsagkas, Jack Rome, Subramanian Ramamoorthy, Oisin Mac Aodha, Chris Xiaoxuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise manipulation that is generalizable across scenes and objects remains
a persistent challenge in robotics. Current approaches for this task heavily
depend on having a significant number of training instances to handle objects
with pronounced visual and/or geometric part ambiguities. Our work explores the
grounding of fine-grained part descriptors for precise manipulation in a
zero-shot setting by utilizing web-trained text-to-image diffusion-based
generative models. We tackle the problem by framing it as a dense semantic part
correspondence task. Our model returns a gripper pose for manipulating a
specific part, using as reference a user-defined click from a source image of a
visually different instance of the same object. We require no manual grasping
demonstrations as we leverage the intrinsic object geometry and features.
Practical experiments in a real-world tabletop scenario validate the efficacy
of our approach, demonstrating its potential for advancing semantic-aware
robotics manipulation. Web page: https://tsagkas.github.io/click2grasp
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Based Causal Reasoning for Safe & Robust Next-Best Action
  Selection in Robot Manipulation Tasks <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Cannizzaro, Michael Groom, Jonathan Routley, Robert Osazuwa Ness, Lars Kunze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe and efficient object manipulation is a key enabler of many real-world
robot applications. However, this is challenging because robot operation must
be robust to a range of sensor and actuator uncertainties. In this paper, we
present a physics-informed causal-inference-based framework for a robot to
probabilistically reason about candidate actions in a block stacking task in a
partially observable setting. We integrate a physics-based simulation of the
rigid-body system dynamics with a causal Bayesian network (CBN) formulation to
define a causal generative probabilistic model of the robot decision-making
process. Using simulation-based Monte Carlo experiments, we demonstrate our
framework's ability to successfully: (1) predict block tower stability with
high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best
action for the block stacking task, for execution by an integrated robot
system, achieving 94.2% task success rate. We also demonstrate our framework's
suitability for real-world robot systems by demonstrating successful task
executions with a domestic support robot, with perception and manipulation
sub-system integration. Hence, we show that by embedding physics-based causal
reasoning into robots' decision-making processes, we can make robot task
execution safer, more reliable, and more robust to various types of
uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bringing Robots Home: The Rise of AI Robots in Consumer Electronics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwei Dong, Yang Liu, Ted Chu, Abdulmotaleb El Saddik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On March 18, 2024, NVIDIA unveiled Project GR00T, a general-purpose
multimodal generative AI model designed specifically for training humanoid
robots. Preceding this event, Tesla's unveiling of the Optimus Gen 2 humanoid
robot on December 12, 2023, underscored the profound impact robotics is poised
to have on reshaping various facets of our daily lives. While robots have long
dominated industrial settings, their presence within our homes is a burgeoning
phenomenon. This can be attributed, in part, to the complexities of domestic
environments and the challenges of creating robots that can seamlessly
integrate into our daily routines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Consumer Electronics Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring 3D Human Pose Estimation and Forecasting from the Robot's
  Perspective: The HARPER <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Avogaro. Andrea Toaiari, Federico Cunico, Xiangmin Xu, Haralambos Dafas, Alessandro Vinciarelli, Emma Li, Marco Cristani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HARPER, a novel dataset for 3D body pose estimation and forecast
in dyadic interactions between users and \spot, the quadruped robot
manufactured by Boston Dynamics. The key-novelty is the focus on the robot's
perspective, i.e., on the data captured by the robot's sensors. These make 3D
body pose analysis challenging because being close to the ground captures
humans only partially. The scenario underlying HARPER includes 15 actions, of
which 10 involve physical contact between the robot and users. The Corpus
contains not only the recordings of the built-in stereo cameras of Spot, but
also those of a 6-camera OptiTrack system (all recordings are synchronized).
This leads to ground-truth skeletal representations with a precision lower than
a millimeter. In addition, the Corpus includes reproducible benchmarks on 3D
Human Pose Estimation, Human Pose Forecasting, and Collision Prediction, all
based on publicly available baseline approaches. This enables future HARPER
users to rigorously compare their results with those we provide in this work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Model Learning and Adaptive Tracking <span class="highlight-title">Control</span> of Magnetic
  Micro-Robots for Non-Contact Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyi Jia, Shu Miao, Junjian Zhou, Niandong Jiao, Lianqing Liu, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic microrobots can be navigated by an external magnetic field to
autonomously move within living organisms with complex and unstructured
environments. Potential applications include drug delivery, diagnostics, and
therapeutic interventions. Existing techniques commonly impart magnetic
properties to the target object,or drive the robot to contact and then
manipulate the object, both probably inducing physical damage. This paper
considers a non-contact formulation, where the robot spins to generate a
repulsive field to push the object without physical contact. Under such a
formulation, the main challenge is that the motion model between the input of
the magnetic field and the output velocity of the target object is commonly
unknown and difficult to analyze. To deal with it, this paper proposes a
data-driven-based solution. A neural network is constructed to efficiently
estimate the motion model. Then, an approximate model-based optimal control
scheme is developed to push the object to track a time-varying trajectory,
maintaining the non-contact with distance constraints. Furthermore, a
straightforward planner is introduced to assess the adaptability of non-contact
manipulation in a cluttered unstructured environment. Experimental results are
presented to show the tracking and navigation performance of the proposed
scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, received by 2024 IEEE International Conference on
  Robotics and Automation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video
  Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoonsung Kim, Changhun Oh, Jinwoo Hwang, Wonung Kim, Seongryong Oh, Yubin Lee, Hardik Sharma, Amir Yazdanbakhsh, Jongse Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural network (DNN) video analytics is crucial for autonomous systems
such as self-driving vehicles, unmanned aerial vehicles (UAVs), and security
robots. However, real-world deployment faces challenges due to their limited
computational resources and battery power. To tackle these challenges,
continuous learning exploits a lightweight "student" model at deployment
(inference), leverages a larger "teacher" model for labeling sampled data
(labeling), and continuously retrains the student model to adapt to changing
scenarios (retraining). This paper highlights the limitations in
state-of-the-art continuous learning systems: (1) they focus on computations
for retraining, while overlooking the compute needs for inference and labeling,
(2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous
systems, and (3) they are located on a remote centralized server, intended for
multi-tenant scenarios, again unsuitable for autonomous systems due to privacy,
network availability, and latency concerns. We propose a hardware-algorithm
co-designed solution for continuous learning, DaCapo, that enables autonomous
systems to perform concurrent executions of inference, labeling, and training
in a performant and energy-efficient manner. DaCapo comprises (1) a
spatially-partitionable and precision-flexible accelerator enabling parallel
execution of kernels on sub-accelerators at their respective precisions, and
(2) a spatiotemporal resource allocation algorithm that strategically navigates
the resource-accuracy tradeoff space, facilitating optimal decisions for
resource allocation to achieve maximal accuracy. Our evaluation shows that
DaCapo achieves 6.5% and 5.5% higher accuracy than a state-of-the-art GPU-based
continuous learning systems, Ekya and EOMU, respectively, while consuming 254x
less power.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of Real-Time Implementable Cooperative Aerial
  Manipulation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stamatina C. Barakou, Costas S. Tzafestas, Kimon P. Valavanis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey paper focuses on quadrotor- and multirotor- based cooperative
aerial manipulation. Emphasis is first given on comparing and evaluating
prototype systems that have been implemented and tested in real-time in diverse
application environments. Underlying modeling and control approaches are also
discussed and compared. The outcome of the survey allows for understanding the
motivation and rationale to develop such systems, their applicability and
implementability in diverse applications and also challenges that need to be
addressed and overcome. Moreover, the survey provides a guide to develop the
next generation of prototype systems based on preferred characteristics,
functionality, operability and application domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MDPI Drones</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tell Me What You Want (What You Really, Really Want): Addressing the
  Expectation Gap for Goal Conveyance from Humans to Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Leahy, Ho Chit Siu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conveying human goals to autonomous systems (AS) occurs both when the system
is being designed and when it is being operated. The design-step conveyance is
typically mediated by robotics and AI engineers, who must appropriately capture
end-user requirements and concepts of operations, while the operation-step
conveyance is mediated by the design, interfaces, and behavior of the AI.
However, communication can be difficult during both these periods because of
mismatches in the expectations and expertise of the end-user and the
roboticist, necessitating more design cycles to resolve. We examine some of the
barriers in communicating system design requirements, and develop an
augmentation for applied cognitive task analysis (ACTA) methods, that we call
robot task analysis (RTA), pertaining specifically to the development of
autonomous systems. Further, we introduce a top-down view of an underexplored
area of friction between requirements communication -- implied human
expectations -- utilizing a collection of work primarily from experimental
psychology and social sciences. We show how such expectations can be used in
conjunction with task-specific expectations and the system design process for
AS to improve design team communication, alleviate barriers to user rejection,
and reduce the number of design cycles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the End-User Development for Human-Robot Interaction
  (EUD4HRI) workshop at HRI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling <span class="highlight-title">Reinforcement Learning</span> Policies for Interpretable Robot
  <span class="highlight-title">Locomotion</span>: Gradient Boosting Machines and Symbolic Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Acero, Zhibin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in reinforcement learning (RL) have led to remarkable
achievements in robot locomotion capabilities. However, the complexity and
``black-box'' nature of neural network-based RL policies hinder their
interpretability and broader acceptance, particularly in applications demanding
high levels of safety and reliability. This paper introduces a novel approach
to distill neural RL policies into more interpretable forms using Gradient
Boosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic
Regression. By leveraging the inherent interpretability of generalized additive
models, decision trees, and analytical expressions, we transform opaque neural
network policies into more transparent ``glass-box'' models. We train expert
neural network policies using RL and subsequently distill them into (i) GBMs,
(ii) EBMs, and (iii) symbolic policies. To address the inherent distribution
shift challenge of behavioral cloning, we propose to use the Dataset
Aggregation (DAgger) algorithm with a curriculum of episode-dependent
alternation of actions between expert and distilled policies, to enable
efficient distillation of feedback control policies. We evaluate our approach
on various robot locomotion gaits -- walking, trotting, bounding, and pacing --
and study the importance of different observations in joint actions for
distilled policies using various methods. We train neural expert policies for
205 hours of simulated experience and distill interpretable policies with only
10 minutes of simulated interaction for each gait using the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation and Deployment of LiDAR-based Place Recognition in Dense
  Forests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haedam Oh, Nived Chebrolu, Matias Mattamala, Leonard Freißmuth, Maurice Fallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many LiDAR place recognition systems have been developed and tested
specifically for urban driving scenarios. Their performance in natural
environments such as forests and woodlands have been studied less closely. In
this paper, we analyzed the capabilities of four different LiDAR place
recognition systems, both handcrafted and learning-based methods, using LiDAR
data collected with a handheld device and legged robot within dense forest
environments. In particular, we focused on evaluating localization where there
is significant translational and orientation difference between corresponding
LiDAR scan pairs. This is particularly important for forest survey systems
where the sensor or robot does not follow a defined road or path. Extending our
analysis we then incorporated the best performing approach, Logg3dNet, into a
full 6-DoF pose estimation system -- introducing several verification layers
for precise registration. We demonstrated the performance of our methods in
three operational modes: online SLAM, offline multi-mission SLAM map merging,
and relocalization into a prior map. We evaluated these modes using data
captured in forests from three different countries, achieving 80% of correct
loop closures candidates with baseline distances up to 5m, and 60% up to 10m.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exosense: A Vision-Centric Scene Understanding System For Safe
  Exoskeleton <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianeng Wang, Matias Mattamala, Christina Kassab, Lintong Zhang, Maurice Fallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exoskeletons for daily use by those with mobility impairments are being
developed. They will require accurate and robust scene understanding systems.
Current research has used vision to identify immediate terrain and geometric
obstacles, however these approaches are constrained to detections directly in
front of the user and are limited to classifying a finite range of terrain
types (e.g., stairs, ramps and level-ground). This paper presents Exosense, a
vision-centric scene understanding system which is capable of generating rich,
globally-consistent elevation maps, incorporating both semantic and terrain
traversability information. It features an elastic Atlas mapping framework
associated with a visual SLAM pose graph, embedded with open-vocabulary room
labels from a Vision-Language Model (VLM). The device's design includes a wide
field-of-view (FoV) fisheye multi-camera system to mitigate the challenges
introduced by the exoskeleton walking pattern. We demonstrate the system's
robustness to the challenges of typical periodic walking gaits, and its ability
to construct accurate semantically-rich maps in indoor settings. Additionally,
we showcase its potential for motion planning -- providing a step towards safe
navigation for exoskeletons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian <span class="highlight-title">Optimization</span> for Sample-Efficient Policy Improvement in Robotic
  Manipulation <span class="chip">IROS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Röfer, Iman Nematollahi, Tim Welschehold, Wolfram Burgard, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sample efficient learning of manipulation skills poses a major challenge in
robotics. While recent approaches demonstrate impressive advances in the type
of task that can be addressed and the sensing modalities that can be
incorporated, they still require large amounts of training data. Especially
with regard to learning actions on robots in the real world, this poses a major
problem due to the high costs associated with both demonstrations and
real-world robot interactions. To address this challenge, we introduce
BOpt-GMM, a hybrid approach that combines imitation learning with own
experience collection. We first learn a skill model as a dynamical system
encoded in a Gaussian Mixture Model from a few demonstrations. We then improve
this model with Bayesian optimization building on a small number of autonomous
skill executions in a sparse reward setting. We demonstrate the sample
efficiency of our approach on multiple complex manipulation skills in both
simulations and real-world experiments. Furthermore, we make the code and
pre-trained models publicly available at http://bopt-gmm. cs.uni-freiburg.de.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 2 tables, submitted to IROS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic
  Supervision <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Hu, Kehan Wen, Fisher Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning dexterous locomotion policy for legged robots is becoming
increasingly popular due to its ability to handle diverse terrains and resemble
intelligent behaviors. However, joint manipulation of moving objects and
locomotion with legs, such as playing soccer, receive scant attention in the
learning community, although it is natural for humans and smart animals. A key
challenge to solve this multitask problem is to infer the objectives of
locomotion from the states and targets of the manipulated objects. The implicit
relation between the object states and robot locomotion can be hard to capture
directly from the training experience. We propose adding a feedback control
block to compute the necessary body-level movement accurately and using the
outputs as dynamic joint-level locomotion supervision explicitly. We further
utilize an improved ball dynamic model, an extended context-aided estimator,
and a comprehensive ball observer to facilitate transferring policy learned in
simulation to the real world. We observe that our learning scheme can not only
make the policy network converge faster but also enable soccer robots to
perform sophisticated maneuvers like sharp cuts and turns on flat surfaces, a
capability that was lacking in previous methods. Video and code are available
at https://github.com/SysCV/soccer-player
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Reactions to Incorrect Answers from Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ponkoj Chandra Shill, Md. Azizul Hakim, Muhammad Jahanzeb Khan, Bashira Akter Anima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As robots grow more and more integrated into numerous industries, it is
critical to comprehend how humans respond to their failures. This paper
systematically studies how trust dynamics and system design are affected by
human responses to robot failures. The three-stage survey used in the study
provides a thorough understanding of human-robot interactions. While the second
stage concentrates on interaction details, such as robot precision and error
acknowledgment, the first stage collects demographic data and initial levels of
trust. In the last phase, participants' perceptions are examined after the
encounter, and trust dynamics, forgiveness, and propensity to suggest robotic
technologies are evaluated. Results show that participants' trust in robotic
technologies increased significantly when robots acknowledged their errors or
limitations to participants and their willingness to suggest robots for
activities in the future points to a favorable change in perception,
emphasizing the role that direct engagement has in influencing trust dynamics.
By providing useful advice for creating more sympathetic, responsive, and
reliable robotic systems, the study advances the science of human-robot
interaction and promotes a wider adoption of robotic technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, 1 table, Ro-Man 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UAV-Assisted Maritime Search and Rescue: A Holistic Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Messmer, Benjamin Kiefer, Leon Amadeus Varga, Andreas Zell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the application of Unmanned Aerial Vehicles (UAVs)
in maritime search and rescue (mSAR) missions, focusing on medium-sized
fixed-wing drones and quadcopters. We address the challenges and limitations
inherent in operating some of the different classes of UAVs, particularly in
search operations. Our research includes the development of a comprehensive
software framework designed to enhance the efficiency and efficacy of SAR
operations. This framework combines preliminary detection onboard UAVs with
advanced object detection at ground stations, aiming to reduce visual strain
and improve decision-making for operators. It will be made publicly available
upon publication. We conduct experiments to evaluate various Region of Interest
(RoI) proposal methods, especially by imposing simulated limited bandwidth on
them, an important consideration when flying remote or offshore operations.
This forces the algorithm to prioritize some predictions over others.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Salzmann, Markus Ryll, Alex Bewley, Matthias Minderer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual relationship detection aims to identify objects and their
relationships in images. Prior methods approach this task by adding separate
relationship modules or decoders to existing object detection architectures.
This separation increases complexity and hinders end-to-end training, which
limits performance. We propose a simple and highly efficient decoder-free
architecture for open-vocabulary visual relationship detection. Our model
consists of a Transformer-based image encoder that represents objects as tokens
and models their relationships implicitly. To extract relationship information,
we introduce an attention mechanism that selects object pairs likely to form a
relationship. We provide a single-stage recipe to train this model on a mixture
of object and relationship detection data. Our approach achieves
state-of-the-art relationship detection performance on Visual Genome and on the
large-vocabulary GQA benchmark at real-time inference speeds. We provide
analyses of zero-shot performance, ablations, and real-world qualitative
examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReFeree: Radar-based efficient global descriptor using a Feature and
  Free space for Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byunghee Choi, Hogyun Kim, Younggun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radar is highlighted for robust sensing capabilities in adverse weather
conditions (e.g. dense fog, heavy rain, or snowfall). In addition, Radar can
cover wide areas and penetrate small particles. Despite these advantages,
Radar-based place recognition remains in the early stages compared to other
sensors due to its unique characteristics such as low resolution, and
significant noise. In this paper, we propose a Radarbased place recognition
utilizing a descriptor called ReFeree using a feature and free space. Unlike
traditional methods, we overwhelmingly summarize the Radar image. Despite being
lightweight, it contains semi-metric information and is also outstanding from
the perspective of place recognition performance. For concrete validation, we
test a single session from the MulRan dataset and a multi-session from the
Oxford Radar RobotCar and the Boreas dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HCTO: Optimality-Aware LiDAR Inertial Odometry with Hybrid Continuous
  Time <span class="highlight-title">Optimization</span> for Compact Wearable Mapping System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianping Li, Shenghai Yuan, Muqing Cao, Thien-Minh Nguyen, Kun Cao, Lihua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compact wearable mapping system (WMS) has gained significant attention due to
their convenience in various applications. Specifically, it provides an
efficient way to collect prior maps for 3D structure inspection and robot-based
"last-mile delivery" in complex environments. However, vibrations in human
motion and the uneven distribution of point cloud features in complex
environments often lead to rapid drift, which is a prevalent issue when
applying existing LiDAR Inertial Odometry (LIO) methods on low-cost WMS. To
address these limitations, we propose a novel LIO for WMSs based on Hybrid
Continuous Time Optimization (HCTO) considering the optimality of Lidar
correspondences. First, HCTO recognizes patterns in human motion
(high-frequency part, low-frequency part, and constant velocity part) by
analyzing raw IMU measurements. Second, HCTO constructs hybrid IMU factors
according to different motion states, which enables robust and accurate
estimation against vibration-induced noise in the IMU measurements. Third, the
best point correspondences are selected using optimal design to achieve
real-time performance and better odometry accuracy. We conduct experiments on
head-mounted WMS datasets to evaluate the performance of our system,
demonstrating significant advantages over state-of-the-art methods. Video
recordings of experiments can be found on the project page of HCTO:
\href{https://github.com/kafeiyin00/HCTO}{https://github.com/kafeiyin00/HCTO}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Model-based Room-Object Relationships
  Knowledge for Enhancing Multimodal-Input Object Goal <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyuan Sun, Asako Kanezaki, Guillaume Caron, Yusuke Yoshiyasu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-goal navigation is a crucial engineering task for the community of
embodied navigation; it involves navigating to an instance of a specified
object category within unseen environments. Although extensive investigations
have been conducted on both end-to-end and modular-based, data-driven
approaches, fully enabling an agent to comprehend the environment through
perceptual knowledge and perform object-goal navigation as efficiently as
humans remains a significant challenge. Recently, large language models have
shown potential in this task, thanks to their powerful capabilities for
knowledge extraction and integration. In this study, we propose a data-driven,
modular-based approach, trained on a dataset that incorporates common-sense
knowledge of object-to-room relationships extracted from a large language
model. We utilize the multi-channel Swin-Unet architecture to conduct
multi-task learning incorporating with multimodal inputs. The results in the
Habitat simulator demonstrate that our framework outperforms the baseline by an
average of 10.6% in the efficiency metric, Success weighted by Path Length
(SPL). The real-world demonstration shows that the proposed approach can
efficiently conduct this task by traversing several rooms. For more details and
real-world demonstrations, please check our project webpage
(https://sunleyuan.github.io/ObjectNav).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>will soon submit to the Elsevier journal, Advanced Engineering
  Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extrinsic Calibration of Multiple LiDARs for a Mobile Robot based on
  Floor Plane And Object Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Niijima, Atsushi Suzuki, Ryoichi Tsuzaki, Masaya Kinoshita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile robots equipped with multiple light detection and ranging (LiDARs) and
capable of recognizing their surroundings are increasing due to the
minitualization and cost reduction of LiDAR. This paper proposes a target-less
extrinsic calibration method of multiple LiDARs with non-overlapping field of
view (FoV). The proposed method uses accumulated point clouds of floor plane
and objects while in motion. It enables accurate calibration with challenging
configuration of LiDARs that directed towards the floor plane, caused by biased
feature values. Additionally, the method includes a noise removal module that
considers the scanning pattern to address bleeding points, which are noises of
significant source of error in point cloud alignment using high-density LiDARs.
Evaluations through simulation demonstrate that the proposed method achieved
higher accuracy extrinsic calibration with two and four LiDARs than
conventional methods, regardless type of objects. Furthermore, the experiments
using a real mobile robot has shown that our proposed noise removal module can
eliminate noise more precisely than conventional methods, and the estimated
extrinsic parameters have successfully created consistent 3D maps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8pages, 10figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development of a Compact Robust Passive Transformable Omni-Ball for
  Enhanced Step-Climbing and Vibration Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuo Hongo, Takashi Kito, Yasuhisa Kamikawa, Masaya Kinoshita, Yasunori Kawanami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Passive Transformable Omni-Ball (PTOB), an advanced
omnidirectional wheel engineered to enhance step-climbing performance,
incorporate built-in actuators, diminish vibrations, and fortify structural
integrity. By modifying the omni-ball's structure from two to three segments,
we have achieved improved in-wheel actuation and a reduction in vibrational
feedback. Additionally, we have implemented a sliding mechanism in the follower
wheels to boost the wheel's step-climbing abilities. A prototype with a 127 mm
diameter PTOB was constructed, which confirmed its functionality for
omnidirectional movement and internal actuation. Compared to a traditional
omni-wheel, the PTOB demonstrated a comparable level of vibration while
offering superior capabilities. Extensive testing in varied settings showed
that the PTOB can adeptly handle step obstacles up to 45 mm, equivalent to 35
$\%$ of the wheel's diameter, in both the forward and lateral directions. The
PTOB showcased robust construction and proved to be versatile in navigating
through environments with diverse obstacles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust <span class="highlight-title">Locomotion</span> via Zero-order Stochastic Nonlinear Model Predictive
  <span class="highlight-title">Control</span> with Guard Saltation Matrix 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sotaro Katayama, Noriaki Takasugi, Mitsuhisa Kaneko, Norio Nagatsuka, and Masaya Kinoshita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a stochastic/robust nonlinear model predictive control
(NMPC) to enhance the robustness of legged locomotion against contact
uncertainties. We integrate the contact uncertainties into the covariance
propagation of stochastic/robust NMPC framework by leveraging the guard
saltation matrix and an extended Kalman filter-like covariance update. We
achieve fast stochastic/robust NMPC computation by utilizing the zero-order
stochastic/robust NMPC algorithm with additional improvements in computational
efficiency concerning the feedback gains. We conducted numerical experiments
and demonstrate that the proposed method can accurately forecast future state
covariance and generate trajectories that satisfies constraints even in the
presence of the contact uncertainties. Hardware experiments on the perceptive
locomotion of a wheeled-legged robot were also carried out, validating the
feasibility of the proposed method in a real-world system with limited on-board
computation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evidential Semantic Mapping in Off-road Environments with
  Uncertainty-aware Bayesian Kernel Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyoung Kim, Junwon Seo, Jihong Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic mapping with Bayesian Kernel Inference (BKI) has shown promise in
creating semantic maps by effectively leveraging local spatial information.
However, existing semantic mapping methods face challenges in constructing
reliable maps in unstructured outdoor scenarios due to unreliable semantic
predictions. To address this issue, we propose an evidential semantic mapping,
which can enhance reliability in perceptually challenging off-road
environments. We integrate Evidential Deep Learning into the semantic
segmentation network to obtain the uncertainty estimate of semantic prediction.
Subsequently, this semantic uncertainty is incorporated into an
uncertainty-aware BKI, tailored to prioritize more confident semantic
predictions when accumulating semantic information. By adaptively handling
semantic uncertainties, the proposed framework constructs robust
representations of the surroundings even in previously unseen environments.
Comprehensive experiments across various off-road datasets demonstrate that our
framework enhances accuracy and robustness, consistently outperforming existing
methods in scenes with high perceptual uncertainties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our project website can be found at
  https://kjyoung.github.io/Homepage/#/Projects/Evidential-Semantic-Mapping</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantics from Space: Satellite-Guided Thermal Semantic Segmentation
  Annotation for Aerial Field Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Lee, Saraswati Soedarmadji, Matthew Anderson, Anthony J. Clark, Soon-Jo Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new method to automatically generate semantic segmentation
annotations for thermal imagery captured from an aerial vehicle by utilizing
satellite-derived data products alongside onboard global positioning and
attitude estimates. This new capability overcomes the challenge of developing
thermal semantic perception algorithms for field robots due to the lack of
annotated thermal field datasets and the time and costs of manual annotation,
enabling precise and rapid annotation of thermal data from field collection
efforts at a massively-parallelizable scale. By incorporating a
thermal-conditioned refinement step with visual foundation models, our approach
can produce highly-precise semantic segmentation labels using low-resolution
satellite land cover data for little-to-no cost. It achieves 98.5% of the
performance from using costly high-resolution options and demonstrates between
70-160% improvement over popular zero-shot semantic segmentation methods based
on large vision-language models currently used for generating annotations for
RGB imagery. Code will be available at:
https://github.com/connorlee77/aerial-auto-segment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Roadmap Towards Automated and Regulated Robotic Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Liu, Mehran Armand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of generative technology opens up possibility for
higher level of automation, and artificial intelligence (AI) embodiment in
robotic systems is imminent. However, due to the blackbox nature of the
generative technology, the generation of the knowledge and workflow scheme is
uncontrolled, especially in a dynamic environment and a complex scene. This
poses challenges to regulations in safety-demanding applications such as
medical scenes. We argue that the unregulated generative processes from AI is
fitted for low level end tasks, but intervention in the form of manual or
automated regulation should happen post-workflow-generation and
pre-robotic-execution. To address this, we propose a roadmap that can lead to
fully automated and regulated robotic systems. In this paradigm, the high level
policies are generated as structured graph data, enabling regulatory oversight
and reusability, while the code base for lower level tasks is generated by
generative models. Our approach aims the transitioning from expert knowledge to
regulated action, akin to the iterative processes of study, practice, scrutiny,
and execution in human tasks. We identify the generative and deterministic
processes in a design cycle, where generative processes serve as a text-based
world simulator and the deterministic processes generate the executable system.
We propose State Machine Seralization Language (SMSL) to be the conversion
point between text simulator and executable workflow control. From there, we
analyze the modules involved based on the current literature, and discuss human
in the loop. As a roadmap, this work identifies the current possible
implementation and future work. This work does not provide an implemented
system but envisions to inspire the researchers working on the direction in the
roadmap. We implement the SMSL and D-SFO paradigm that serve as the starting
point of the roadmap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> TD-<span class="highlight-title">MPC</span>2: Scalable, Robust World Models for Continuous <span class="highlight-title">Control</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicklas Hansen, Hao Su, <span class="highlight-author">Xiaolong Wang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TD-MPC is a model-based reinforcement learning (RL) algorithm that performs
local trajectory optimization in the latent space of a learned implicit
(decoder-free) world model. In this work, we present TD-MPC2: a series of
improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves
significantly over baselines across 104 online RL tasks spanning 4 diverse task
domains, achieving consistently strong results with a single set of
hyperparameters. We further show that agent capabilities increase with model
and data size, and successfully train a single 317M parameter agent to perform
80 tasks across multiple task domains, embodiments, and action spaces. We
conclude with an account of lessons, opportunities, and risks associated with
large TD-MPC2 agents. Explore videos, models, data, code, and more at
https://tdmpc2.com
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024. Explore videos, models, data, code, and more at
  https://tdmpc2.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Modular Aerial System Based on Homogeneous Quadrotors with
  Fault-Tolerant <span class="highlight-title">Control</span> <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengguang Li, Kai Cui, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The standard quadrotor is one of the most popular and widely used aerial
vehicle of recent decades, offering great maneuverability with mechanical
simplicity. However, the under-actuation characteristic limits its
applications, especially when it comes to generating desired wrench with six
degrees of freedom (DOF). Therefore, existing work often compromises between
mechanical complexity and the controllable DOF of the aerial system. To take
advantage of the mechanical simplicity of a standard quadrotor, we propose a
modular aerial system, IdentiQuad, that combines only homogeneous
quadrotor-based modules. Each IdentiQuad can be operated alone like a standard
quadrotor, but at the same time allows task-specific assembly, increasing the
controllable DOF of the system. Each module is interchangeable within its
assembly. We also propose a general controller for different configurations of
assemblies, capable of tolerating rotor failures and balancing the energy
consumption of each module. The functionality and robustness of the system and
its controller are validated using physics-based simulations for different
assembly configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance-aware Exploration-Verification-Exploitation for Instance
  ImageGoal <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Lei, Min Wang, Wengang Zhou, Li Li, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to
navigate to a specified object depicted by a goal image in an unexplored
environment.
  The main challenge of this task lies in identifying the target object from
different viewpoints while rejecting similar distractors.
  Existing ImageGoal Navigation methods usually adopt the simple
Exploration-Exploitation framework and ignore the identification of specific
instance during navigation.
  In this work, we propose to imitate the human behaviour of ``getting closer
to confirm" when distinguishing objects from a distance.
  Specifically, we design a new modular navigation framework named
Instance-aware Exploration-Verification-Exploitation (IEVE) for instance-level
image goal navigation.
  Our method allows for active switching among the exploration, verification,
and exploitation actions, thereby facilitating the agent in making reasonable
decisions under different situations.
  On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our
method surpasses previous state-of-the-art work, with a classical segmentation
model (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success).
Our code will be made publicly available at https://github.com/XiaohanLei/IEVE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning a Depth Covariance Function <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Dexheimer, Andrew J. Davison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose learning a depth covariance function with applications to
geometric vision tasks. Given RGB images as input, the covariance function can
be flexibly used to define priors over depth functions, predictive
distributions given observations, and methods for active point selection. We
leverage these techniques for a selection of downstream tasks: depth
completion, bundle adjustment, and monocular dense visual odometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page: https://edexheim.github.io/DepthCov/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning reduces sensor requirements for gust rejection on a small
  uncrewed aerial vehicle morphing wing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin PT. Haughn, Christina Harvey, Daniel J. Inman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing need for uncrewed aerial vehicles (UAVs) to operate in
cities. However, the uneven urban landscape and complex street systems cause
large-scale wind gusts that challenge the safe and effective operation of UAVs.
Current gust alleviation methods rely on traditional control surfaces and
computationally expensive modeling to select a control action, leading to a
slower response. Here, we used deep reinforcement learning to create an
autonomous gust alleviation controller for a camber-morphing wing. This method
reduced gust impact by 84%, directly from real-time, on-board pressure signals.
Notably, we found that gust alleviation using signals from only three pressure
taps was statistically indistinguishable from using six signals. This
reduced-sensor fly-by-feel control opens the door to UAV missions in previously
inoperable locations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Human's Gender Perception and Bias toward Non-Humanoid Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12001v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12001v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahya Ramezani, Jose Luis Sanchez-Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As non-humanoid robots increasingly permeate various sectors, understanding
their design implications for human acceptance becomes paramount. Despite their
ubiquity, studies on how to improve human interaction are sparse. Our
investigation, conducted through two surveys, addresses this gap. The first
survey emphasizes non-humanoid robots and human perceptions about gender
attributions, suggesting that both design and perceived gender influence
acceptance. Survey 2 investigates the effects of varying gender cues on robot
designs and their consequent impacts on human-robot interactions. Our findings
highlighted that distinct gender cues can bolster or impede interaction
comfort.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Star-Searcher: A Complete and Efficient Aerial System for Autonomous
  Target Search in Complex Unknown Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Luo, Zixuan Zhuang, Neng Pan, Chen Feng, Shaojie Shen, Fei Gao, Hui Cheng, Boyu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the challenge of autonomous target search using unmanned
aerial vehicles (UAVs) in complex unknown environments. To fill the gap in
systematic approaches for this task, we introduce Star-Searcher, an aerial
system featuring specialized sensor suites, mapping, and planning modules to
optimize searching. Path planning challenges due to increased inspection
requirements are addressed through a hierarchical planner with a
visibility-based viewpoint clustering method. This simplifies planning by
breaking it into global and local sub-problems, ensuring efficient global and
local path coverage in real-time. Furthermore, our global path planning employs
a history-aware mechanism to reduce motion inconsistency from frequent map
changes, significantly enhancing search efficiency. We conduct comparisons with
state-of-the-art methods in both simulation and the real world, demonstrating
shorter flight paths, reduced time, and higher target search completeness. Our
approach will be open-sourced for community benefit at
https://github.com/SYSU-STAR/STAR-Searcher.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Aceepted to IEEE RA-L. Code:
  https://github.com/SYSU-STAR/STAR-Searcher. Video:
  https://www.youtube.com/watch?v=08ll_oo_DtU</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models for Multi-Modal Human-Robot Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Stephan Hasler, Daniel Tanneberg, Felix Ocker, Frank Joublin, Antonello Ceravola, Joerg Deigmoeller, Michael Gienger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an innovative large language model (LLM)-based robotic
system for enhancing multi-modal human-robot interaction (HRI). Traditional HRI
systems relied on complex designs for intent estimation, reasoning, and
behavior generation, which were resource-intensive. In contrast, our system
empowers researchers and practitioners to regulate robot behavior through three
key aspects: providing high-level linguistic guidance, creating "atomics" for
actions and expressions the robot can use, and offering a set of examples.
Implemented on a physical robot, it demonstrates proficiency in adapting to
multi-modal inputs and determining the appropriate manner of action to assist
humans with its arms, following researchers' defined guidelines.
Simultaneously, it coordinates the robot's lid, neck, and ear movements with
speech output to produce dynamic, multi-modal expressions. This showcases the
system's potential to revolutionize HRI by shifting from conventional, manual
state-and-flow design methods to an intuitive, guidance-based, and
example-driven approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAkEable: Memory-centered and Affordance-based Task Execution Framework
  for Transferable Mobile Manipulation Skills 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Pohl, Fabian Reister, Fabian Peller-Konrad, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To perform versatile mobile manipulation tasks in human-centered
environments, the ability to efficiently transfer learned tasks and experiences
from one robot to another or across different environments is key. In this
paper, we present MAkEable, a versatile uni- and multi-manual mobile
manipulation framework that facilitates the transfer of capabilities and
knowledge across different tasks, environments, and robots. Our framework
integrates an affordance-based task description into the memory-centric
cognitive architecture of the ARMAR humanoid robot family, which supports the
sharing of experiences and demonstrations for transfer learning. By
representing mobile manipulation actions through affordances, i.e., interaction
possibilities of the robot with its environment, we provide a unifying
framework for the autonomous uni- and multi-manual manipulation of known and
unknown objects in various environments. We demonstrate the applicability of
the framework in real-world experiments for multiple robots, tasks, and
environments. This includes grasping known and unknown objects, object placing,
bimanual object grasping, memory-enabled skill transfer in a drawer opening
scenario across two different humanoid robots, and a pouring task learned from
human demonstration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Driving Animatronic Robot Facial Expression From Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boren Li, Hang Li, Hangxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animatronic robots aim to enable natural human-robot interaction through
lifelike facial expressions. However, generating realistic, speech-synchronized
robot expressions is challenging due to the complexities of facial biomechanics
and responsive motion synthesis. This paper presents a principled,
skinning-centric approach to drive animatronic robot facial expressions from
speech. The proposed approach employs linear blend skinning (LBS) as the core
representation to guide tightly integrated innovations in embodiment design and
motion synthesis. LBS informs the actuation topology, enables human expression
retargeting, and allows speech-driven facial motion generation. The proposed
approach is capable of generating highly realistic, real-time facial
expressions from speech on an animatronic face, significantly advancing robots'
ability to replicate nuanced human expressions for natural interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. For associated project page, see
  https://library87.github.io/animatronic-face-iros24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot
  Interaction Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16068v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16068v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Castri, Gloria Beraldo, Sariah Mghames, Marc Hanheide, Nicola Bellotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying robots in human-shared spaces requires understanding interactions
among nearby agents and objects. Modelling cause-and-effect relations through
causal inference aids in predicting human behaviours and anticipating robot
interventions. However, a critical challenge arises as existing causal
discovery methods currently lack an implementation inside the ROS ecosystem,
the standard de facto in robotics, hindering effective utilisation in robotics.
To address this gap, this paper introduces ROS-Causal, a ROS-based framework
for onboard data collection and causal discovery in human-robot spatial
interactions. An ad-hoc simulator, integrated with ROS, illustrates the
approach's effectiveness, showcasing the robot onboard generation of causal
models during data collection. ROS-Causal is available on GitHub:
https://github.com/lcastri/roscausal.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the "Causal-HRI: Causal Learning for Human-Robot
  Interaction" workshop at the 2024 ACM/IEEE International Conference on
  Human-Robot Interaction (HRI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Large Language Models to Facilitate Variable Autonomy for
  Human-Robot Teaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07214v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07214v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younes Lakhnati, Max Pascher, Jens Gerken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a rapidly evolving digital landscape autonomous tools and robots are
becoming commonplace. Recognizing the significance of this development, this
paper explores the integration of Large Language Models (LLMs) like Generative
pre-trained transformer (GPT) into human-robot teaming environments to
facilitate variable autonomy through the means of verbal human-robot
communication. In this paper, we introduce a novel framework for such a
GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality
(VR) setting. This system allows users to interact with robot agents through
natural language, each powered by individual GPT cores. By means of OpenAI's
function calling, we bridge the gap between unstructured natural language input
and structure robot actions. A user study with 12 participants explores the
effectiveness of GPT-4 and, more importantly, user strategies when being given
the opportunity to converse in natural language within a multi-robot
environment. Our findings suggest that users may have preconceived expectations
on how to converse with robots and seldom try to explore the actual language
and cognitive capabilities of their robot collaborators. Still, those users who
did explore where able to benefit from a much more natural flow of
communication and human-like back-and-forth. We provide a set of lessons
learned for future research and technical implementations of similar systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Frontiers in Robotics and AI, Variable Autonomy for Human-Robot
  Teaming</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLIM: Skill Learning with Multiple Critics <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Emukpere, Bingbing Wu, Julien Perez, Jean-Michel Renders
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised skill learning aims to acquire useful behaviors that leverage
the underlying dynamics of the environment. Latent variable models, based on
mutual information maximization, have been successful in this task but still
struggle in the context of robotic manipulation. As it requires impacting a
possibly large set of degrees of freedom composing the environment, mutual
information maximization fails alone in producing useful and safe manipulation
behaviors. Furthermore, tackling this by augmenting skill discovery rewards
with additional rewards through a naive combination might fail to produce
desired behaviors. To address this limitation, we introduce SLIM, a
multi-critic learning approach for skill discovery with a particular focus on
robotic manipulation. Our main insight is that utilizing multiple critics in an
actor-critic framework to gracefully combine multiple reward functions leads to
a significant improvement in latent-variable skill discovery for robotic
manipulation while overcoming possible interference occurring among rewards
which hinders convergence to useful skills. Furthermore, in the context of
tabletop manipulation, we demonstrate the applicability of our novel skill
discovery approach to acquire safe and efficient motor primitives in a
hierarchical reinforcement learning fashion and leverage them through planning,
significantly surpassing baseline approaches for skill discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based
  Robots Ecosystems via Proposal Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Antonazzi, Matteo Luperto, N. Alberto Borghese, Nicola Basilico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel approach for scalable domain adaptation in cloud
robotics scenarios where robots rely on third-party AI inference services
powered by large pre-trained deep neural networks. Our method is based on a
downstream proposal-refinement stage running locally on the robots, exploiting
a new lightweight DNN architecture, R2SNet. This architecture aims to mitigate
performance degradation from domain shifts by adapting the object detection
process to the target environment, focusing on relabeling, rescoring, and
suppression of bounding-box proposals. Our method allows for local execution on
robots, addressing the scalability challenges of domain adaptation without
incurring significant computational costs. Real-world results on mobile service
robots performing door detection show the effectiveness of the proposed method
in achieving scalable domain adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoBRA: A Composable Benchmark for Robotics Applications <span class="chip">ICRA'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09337v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09337v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Mayer, Jonathan Külz, Matthias Althoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selecting an optimal robot, its base pose, and trajectory for a given task is
currently mainly done by human expertise or trial and error. To evaluate
automatic approaches to this combined optimization problem, we introduce a
benchmark suite encompassing a unified format for robots, environments, and
task descriptions. Our benchmark suite is especially useful for modular robots,
where the multitude of robots that can be assembled creates a host of
additional parameters to optimize. We include tasks such as machine tending and
welding in synthetic environments and 3D scans of real-world machine shops. All
benchmarks are accessible through https://cobra.cps.cit.tum.de, a platform to
conveniently share, reference, and compare tasks, robot models, and solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 Figures, 5 Tables Final version for IEEE ICRA'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Early Stopping in Evolutionary Direct Policy Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etor Arza, Leni K. Le Goff, Emma Hart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lengthy evaluation times are common in many optimization problems such as
direct policy search tasks, especially when they involve conducting evaluations
in the physical world, e.g. in robotics applications. Often when evaluating
solution over a fixed time period it becomes clear that the objective value
will not increase with additional computation time (for example when a two
wheeled robot continuously spins on the spot). In such cases, it makes sense to
stop the evaluation early to save computation time. However, most approaches to
stop the evaluation are problem specific and need to be specifically designed
for the task at hand. Therefore, we propose an early stopping method for direct
policy search. The proposed method only looks at the objective value at each
time step and requires no problem specific knowledge. We test the introduced
stopping criterion in five direct policy search environments drawn from games,
robotics and classic control domains, and show that it can save up to 75% of
the computation time. We also compare it with problem specific stopping
criteria and show that it performs comparably, while being more generally
applicable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language and Sketching: An LLM-driven Interactive Multimodal Multitask
  Robot <span class="highlight-title">Navigation</span> Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08244v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08244v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiqin Zu, Wenbin Song, Ruiqing Chen, Ze Guo, Fanglei Sun, Zheng Tian, Wei Pan, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The socially-aware navigation system has evolved to adeptly avoid various
obstacles while performing multiple tasks, such as point-to-point navigation,
human-following, and -guiding. However, a prominent gap persists: in
Human-Robot Interaction (HRI), the procedure of communicating commands to
robots demands intricate mathematical formulations. Furthermore, the transition
between tasks does not quite possess the intuitive control and user-centric
interactivity that one would desire. In this work, we propose an LLM-driven
interactive multimodal multitask robot navigation framework, termed LIM2N, to
solve the above new challenge in the navigation field. We achieve this by first
introducing a multimodal interaction framework where language and hand-drawn
inputs can serve as navigation constraints and control objectives. Next, a
reinforcement learning agent is built to handle multiple tasks with the
received information. Crucially, LIM2N creates smooth cooperation among the
reasoning of multimodal input, multitask planning, and adaptation and
processing of the intelligent sensing modules in the complicated system.
Extensive experiments are conducted in both simulation and the real world
demonstrating that LIM2N has superior user needs understanding, alongside an
enhanced interactive experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-time Perceptive Motion <span class="highlight-title">Control</span> using <span class="highlight-title">Control</span> Barrier Functions with
  Analytical Smoothing for Six-Wheeled-Telescopic-<span class="highlight-title">Legged Robot</span> Tachyon 3 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noriaki Takasugi, Masaya Kinoshita, Yasuhisa Kamikawa, Ryoichi Tsuzaki, Atsushi Sakamoto, Toshimitsu Kai, Yasunori Kawanami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve safe legged locomotion, it is important to generate motion in
real-time considering various constraints in robots and environments. In this
study, we propose a lightweight real-time perspective motion control system for
the newly developed six-wheeled-telescopic-legged robot, Tachyon 3. In the
proposed method, analytically smoothed constraints including Smooth Separating
Axis Theorem (Smooth SAT) as a novel higher order differentiable collision
detection for 3D shapes is applied to the Control Barrier Function (CBF). The
proposed system integrating the CBF achieves online motion generation in a
short control cycle of 1 ms that satisfies joint limitations, environmental
collision avoidance and safe convex foothold constraints. The efficiency of
Smooth SAT is shown from the collision detection time of 1 us or less and the
CBF constraint computation time for Tachyon3 of several us. Furthermore, the
effectiveness of the proposed system is verified through the stair-climbing
motion, integrating online recognition in a simulation and a real machine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling and Retrieving Generalizable Knowledge for Robot Manipulation
  via Language Corrections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihan Zha, Yuchen Cui, Li-Heng Lin, Minae Kwon, Montserrat Gonzalez Arenas, Andy Zeng, Fei Xia, Dorsa Sadigh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's robot policies exhibit subpar performance when faced with the
challenge of generalizing to novel environments. Human corrective feedback is a
crucial form of guidance to enable such generalization. However, adapting to
and learning from online human corrections is a non-trivial endeavor: not only
do robots need to remember human feedback over time to retrieve the right
information in new settings and reduce the intervention rate, but also they
would need to be able to respond to feedback that can be arbitrary corrections
about high-level human preferences to low-level adjustments to skill
parameters. In this work, we present Distillation and Retrieval of Online
Corrections (DROC), a large language model (LLM)-based system that can respond
to arbitrary forms of language feedback, distill generalizable knowledge from
corrections, and retrieve relevant past experiences based on textual and visual
similarity for improving performance in novel settings. DROC is able to respond
to a sequence of online language corrections that address failures in both
high-level task plans and low-level skill primitives. We demonstrate that DROC
effectively distills the relevant information from the sequence of online
corrections in a knowledge base and retrieves that knowledge in settings with
new task or object instances. DROC outperforms other techniques that directly
generate robot code via LLMs by using only half of the total number of
corrections needed in the first round and requires little to no corrections
after two iterations. We show further results, videos, prompts and code on
https://sites.google.com/stanford.edu/droc .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, videos and code links on website
  https://sites.google.com/stanford.edu/droc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Inertial Positioning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03757v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03757v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changhao Chen, Xianfei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inertial sensors are widely utilized in smartphones, drones, robots, and IoT
devices, playing a crucial role in enabling ubiquitous and reliable
localization. Inertial sensor-based positioning is essential in various
applications, including personal navigation, location-based security, and
human-device interaction. However, low-cost MEMS inertial sensors' measurements
are inevitably corrupted by various error sources, leading to unbounded drifts
when integrated doubly in traditional inertial navigation algorithms,
subjecting inertial positioning to the problem of error drifts. In recent
years, with the rapid increase in sensor data and computational power, deep
learning techniques have been developed, sparking significant research into
addressing the problem of inertial positioning. Relevant literature in this
field spans across mobile computing, robotics, and machine learning. In this
article, we provide a comprehensive review of deep learning-based inertial
positioning and its applications in tracking pedestrians, drones, vehicles, and
robots. We connect efforts from different fields and discuss how deep learning
can be applied to address issues such as sensor calibration, positioning error
drift reduction, and multi-sensor fusion. This article aims to attract readers
from various backgrounds, including researchers and practitioners interested in
the potential of deep learning-based techniques to solve inertial positioning
problems. Our review demonstrates the exciting possibilities that deep learning
brings to the table and provides a roadmap for future research in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Intelligent Transportation Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMP: Autoregressive Motion Prediction Revisited with Next Token
  Prediction for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosong Jia, Shaoshuai Shi, Zijun Chen, Li Jiang, Wenlong Liao, Tao He, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an essential task in autonomous driving (AD), motion prediction aims to
predict the future states of surround objects for navigation. One natural
solution is to estimate the position of other agents in a step-by-step manner
where each predicted time-step is conditioned on both observed time-steps and
previously predicted time-steps, i.e., autoregressive prediction. Pioneering
works like SocialLSTM and MFP design their decoders based on this intuition.
However, almost all state-of-the-art works assume that all predicted time-steps
are independent conditioned on observed time-steps, where they use a single
linear layer to generate positions of all time-steps simultaneously. They
dominate most motion prediction leaderboards due to the simplicity of training
MLPs compared to autoregressive networks.
  In this paper, we introduce the GPT style next token prediction into motion
forecasting. In this way, the input and output could be represented in a
unified space and thus the autoregressive prediction becomes more feasible.
However, different from language data which is composed of homogeneous units
-words, the elements in the driving scene could have complex spatial-temporal
and semantic relations. To this end, we propose to adopt three factorized
attention modules with different neighbors for information aggregation and
different position encoding styles to capture their relations, e.g., encoding
the transformation between coordinate systems for spatial relativity while
adopting RoPE for temporal relativity. Empirically, by equipping with the
aforementioned tailored designs, the proposed method achieves state-of-the-art
performance in the Waymo Open Motion and Waymo Interaction datasets. Notably,
AMP outperforms other recent autoregressive motion prediction methods: MotionLM
and StateTransformer, which demonstrates the effectiveness of the proposed
designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Redundancy parameterization and inverse kinematics of 7-DOF revolute
  manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.13122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.13122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander J. Elias, John T. Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seven degree-of-freedom (DOF) robot arms have one redundant DOF which does
not change the motion of the end effector. The redundant DOF offers greater
manipulability of the arm configuration to avoid obstacles and singularities,
but it must be parameterized to fully specify the joint angles for a given end
effector pose. For 7-DOF revolute (7R) manipulators, we introduce a new concept
of generalized shoulder-elbow-wrist (SEW) angle, a generalization of the
conventional SEW angle but with an arbitrary choice of the reference direction
function. The SEW angle is widely used and easy for human operators to
visualize as a rotation of the elbow about the shoulder-wrist line. Since other
redundancy parameterizations including the conventional SEW angle encounter an
algorithmic singularity along a line in the workspace, we introduce a special
choice of the reference direction function called the stereographic SEW angle
which has a singularity only along a half-line, which can be placed out of
reach. We prove that such a singularity is unavoidable for any
parameterization. We also include expressions for the SEW angle Jacobian along
with singularity analysis. Finally, we provide efficient and singularity-robust
inverse kinematics solutions for most known 7R manipulators using the general
SEW angle and the subproblem decomposition method. These solutions are often
closed-form but may sometimes involve a 1D or 2D search in the general case.
Search-based solutions may be converted to finding zeros of a high-order
polynomial. Inverse kinematics solutions, examples, and evaluations are
available in a publicly accessible repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 14 figures. Update: Sawyer IK using polynomial method, two
  video extensions, expanded related literature</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Open X-Embodiment: Robotic Learning <span class="highlight-title">Dataset</span>s and RT-X Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08864v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08864v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Open X-Embodiment Collaboration, Abby O'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, <span class="highlight-author">Lerrel Pinto</span>, Li Fei-Fei, Liam Tan, Linxi "Jim" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick "Tree" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, <span class="highlight-author">Sergey Levine</span>, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, <span class="highlight-author">Xiaolong Wang</span>, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large, high-capacity models trained on diverse datasets have shown remarkable
successes on efficiently tackling downstream applications. In domains from NLP
to Computer Vision, this has led to a consolidation of pretrained models, with
general pretrained backbones serving as a starting point for many applications.
Can such a consolidation happen in robotics? Conventionally, robotic learning
methods train a separate model for every application, every robot, and even
every environment. Can we instead train generalist X-robot policy that can be
adapted efficiently to new robots, tasks, and environments? In this paper, we
provide datasets in standardized data formats and models to make it possible to
explore this possibility in the context of robotic manipulation, alongside
experimental results that provide an example of effective X-robot policies. We
assemble a dataset from 22 different robots collected through a collaboration
between 21 institutions, demonstrating 527 skills (160266 tasks). We show that
a high-capacity model trained on this data, which we call RT-X, exhibits
positive transfer and improves the capabilities of multiple robots by
leveraging experience from other platforms. More details can be found on the
project website https://robotics-transformer-x.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://robotics-transformer-x.github.io</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-20T00:00:00Z">2024-03-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">70</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "It's Not a Replacement:" Enabling Parent-Robot Collaboration to Support
  In-Home Learning Experiences of Young Children 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui-Ru Ho, Edward Hubbard, Bilge Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning companion robots for young children are increasingly adopted in
informal learning environments. Although parents play a pivotal role in their
children's learning, very little is known about how parents prefer to
incorporate robots into their children's learning activities. We developed
prototype capabilities for a learning companion robot to deliver educational
prompts and responses to parent-child pairs during reading sessions and
conducted in-home user studies involving 10 families with children aged 3-5.
Our data indicates that parents want to work with robots as collaborators to
augment parental activities to foster children's learning, introducing the
notion of parent-robot collaboration. Our findings offer an empirical
understanding of the needs and challenges of parent-child interaction in
informal learning scenarios and design opportunities for integrating a
companion robot into these interactions. We offer insights into how robots
might be designed to facilitate parent-robot collaboration, including parenting
policies, collaboration patterns, and interaction paradigms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quadcopter Team Configurable Motion Guided by a <span class="highlight-title">Quadruped</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Ghufran, Sourish Tetakayala, Jack Hughes, Aron Wilson, Hossein Rastgoftar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper focuses on modeling and experimental evaluation of a quadcopter
team configurable coordination guided by a single quadruped robot. We consider
the quadcopter team as particles of a two-dimensional deformable body and
propose a two-dimensional affine transformation model for safe and
collision-free configurable coordination of this heterogeneous robotic system.
The proposed affine transformation is decomposed into translation, that is
specified by the quadruped global position, and configurable motion of the
quadcopters, which is determined by a nonsingular Jacobian matrix so that the
quadcopter team can safely navigate a constrained environment while avoiding
collision. We propose two methods to experimentally evaluate the proposed
heterogeneous robot coordination model. The first method measures real
positions of quadcopters, quadruped, and environmental objects all with respect
to the global coordinate system. On the other hand, the second method measures
position with respect to the local coordinate system fixed on the dog robot
which in turn enables safe planning the Jacobian matrix of the quadcopter team
while the world is virtually approached the robotic system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HRI Curriculum for a Liberal Arts Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason R. Wilson, Emily Jensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we discuss the opportunities and challenges of teaching a
human-robot interaction course at an undergraduate liberal arts college. We
provide a sample syllabus adapted from a previous version of a course.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Designing an Intro to HRI Course Workshop at HRI
  2024 (arXiv:2403.05588)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crowdsourcing Task Traces for Service Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Porfirio, Allison Sauppé, Maya Cakmak, Aws Albarghouthi, Bilge Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Demonstration is an effective end-user development paradigm for teaching
robots how to perform new tasks. In this paper, we posit that demonstration is
useful not only as a teaching tool, but also as a way to understand and assist
end-user developers in thinking about a task at hand. As a first step toward
gaining this understanding, we constructed a lightweight web interface to
crowdsource step-by-step instructions of common household tasks, leveraging the
imaginations and past experiences of potential end-user developers. As evidence
of the utility of our interface, we deployed the interface on Amazon Mechanical
Turk and collected 207 task traces that span 18 different task categories. We
describe our vision for how these task traces can be operationalized as task
models within end-user development tools and provide a roadmap for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the companion proceedings of the 2023 ACM/IEEE
  International Conference on Human-Robot Interaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Imitation Learning of Task-Oriented Object Grasping and
  Rearrangement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Cai, Jianfeng Gao, Christoph Pohl, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented object grasping and rearrangement are critical skills for
robots to accomplish different real-world manipulation tasks. However, they
remain challenging due to partial observations of the objects and shape
variations in categorical objects. In this paper, we propose the Multi-feature
Implicit Model (MIMO), a novel object representation that encodes multiple
spatial features between a point and an object in an implicit neural field.
Training such a model on multiple features ensures that it embeds the object
shapes consistently in different aspects, thus improving its performance in
object shape reconstruction from partial observation, shape similarity measure,
and modeling spatial relations between objects. Based on MIMO, we propose a
framework to learn task-oriented object grasping and rearrangement from single
or multiple human demonstration videos. The evaluations in simulation show that
our approach outperforms the state-of-the-art methods for multi- and
single-view observations. Real-world experiments demonstrate the efficacy of
our approach in one- and few-shot imitation learning of manipulation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Goal-Oriented End-User Programming of Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Porfirio, Mark Roberts, Laura M. Hiatt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-user programming (EUP) tools must balance user control with the robot's
ability to plan and act autonomously. Many existing task-oriented EUP tools
enforce a specific level of control, e.g., by requiring that users hand-craft
detailed sequences of actions, rather than offering users the flexibility to
choose the level of task detail they wish to express. We thereby created a
novel EUP system, Polaris, that in contrast to most existing EUP tools, uses
goal predicates as the fundamental building block of programs. Users can
thereby express high-level robot objectives or lower-level checkpoints at their
choosing, while an off-the-shelf task planner fills in any remaining program
detail. To ensure that goal-specified programs adhere to user expectations of
robot behavior, Polaris is equipped with a Plan Visualizer that exposes the
planner's output to the user before runtime. In what follows, we describe our
design of Polaris and its evaluation with 32 human participants. Our results
support the Plan Visualizer's ability to help users craft higher-quality
programs. Furthermore, there are strong associations between user perception of
the robot and Plan Visualizer usage, and evidence that robot familiarity has a
key role in shaping user experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the proceedings of the 2024 ACM/IEEE International
  Conference on Human-Robot Interaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open Access NAO (OAN): a ROS2-based software framework for HRI
  applications with the NAO robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Bono, Kenji Brameld, Luigi D'Alfonso, Giuseppe Fedele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new software framework for HRI experimentation with the
sixth version of the common NAO robot produced by the United Robotics Group.
Embracing the common demand of researchers for better performance and new
features for NAO, the authors took advantage of the ability to run ROS2 onboard
on the NAO to develop a framework independent of the APIs provided by the
manufacturer. Such a system provides NAO with not only the basic skills of a
humanoid robot such as walking and reproducing movements of interest but also
features often used in HRI such as: speech recognition/synthesis, face and
object detention, and the use of Generative Pre-trained Transformer (GPT)
models for conversation. The developed code is therefore configured as a
ready-to-use but also highly expandable and improvable tool thanks to the
possibilities provided by the ROS community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sensory Glove-Based Surgical Robot User Interface <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Borgioli, Ki-Hwan Oh, Alberto Mangano, Alvaro Ducas, Luciano Ambrosini, Federico Pinto, Paula A Lopez, Jessica Cassiani, Milos Zefran, Liaohai Chen, Pier Cristoforo Giulianotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic surgery has reached a high level of maturity and has become an
integral part of standard surgical care. However, existing surgeon consoles are
bulky and take up valuable space in the operating room, present challenges for
surgical team coordination, and their proprietary nature makes it difficult to
take advantage of recent technological advances, especially in virtual and
augmented reality. One potential area for further improvement is the
integration of modern sensory gloves into robotic platforms, allowing surgeons
to control robotic arms directly with their hand movements intuitively. We
propose one such system that combines an HTC Vive tracker, a Manus Meta Prime 3
XR sensory glove, and God Vision wireless smart glasses. The system controls
one arm of a da Vinci surgical robot. In addition to moving the arm, the
surgeon can use fingers to control the end-effector of the surgical instrument.
Hand gestures are used to implement clutching and similar functions. In
particular, we introduce clutching of the instrument orientation, a
functionality not available in the da Vinci system. The vibrotactile elements
of the glove are used to provide feedback to the user when gesture commands are
invoked. A preliminary evaluation of the system shows that it has excellent
tracking accuracy and allows surgeons to efficiently perform common surgical
training tasks with minimal practice with the new interface; this suggests that
the interface is highly intuitive. The proposed system is inexpensive, allows
rapid prototyping, and opens opportunities for further innovations in the
design of surgical robot interfaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, 7 tables, submitted to International Conference
  on Intelligent Robots and Systems (IROS)2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safety-Aware Perception for Autonomous Collision Avoidance in Dynamic
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan M. Bena, Chongbo Zhao, Quan Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous collision avoidance requires accurate environmental perception;
however, flight systems often possess limited sensing capabilities with
field-of-view (FOV) restrictions. To navigate this challenge, we present a
safety-aware approach for online determination of the optimal sensor-pointing
direction $\psi_\text{d}$ which utilizes control barrier functions (CBFs).
First, we generate a spatial density function $\Phi$ which leverages CBF
constraints to map the collision risk of all local coordinates. Then, we
convolve $\Phi$ with an attitude-dependent sensor FOV quality function to
produce the objective function $\Gamma$ which quantifies the total observed
risk for a given pointing direction. Finally, by finding the global optimizer
for $\Gamma$, we identify the value of $\psi_\text{d}$ which maximizes the
perception of risk within the FOV. We incorporate $\psi_\text{d}$ into a
safety-critical flight architecture and conduct a numerical analysis using
multiple simulated mission profiles. Our algorithm achieves a success rate of
$88-96\%$, constituting a $16-29\%$ improvement compared to the best heuristic
methods. We demonstrate the functionality of our approach via a flight
demonstration using the Crazyflie 2.1 micro-quadrotor. Without a priori
obstacle knowledge, the quadrotor follows a dynamic flight path while
simultaneously calculating and tracking $\psi_\text{d}$ to perceive and avoid
two static obstacles with an average computation time of 371 $\mu$s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmented Reality Demonstrations for Scalable Robot Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Yang, Bryce Ikeda, Gedas Bertasius, Daniel Szafir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot Imitation Learning (IL) is a widely used method for training robots to
perform manipulation tasks that involve mimicking human demonstrations to
acquire skills. However, its practicality has been limited due to its
requirement that users be trained in operating real robot arms to provide
demonstrations. This paper presents an innovative solution: an Augmented
Reality (AR)-assisted framework for demonstration collection, empowering
non-roboticist users to produce demonstrations for robot IL using devices like
the HoloLens 2. Our framework facilitates scalable and diverse demonstration
collection for real-world tasks. We validate our approach with experiments on
three classical robotics tasks: reach, push, and pick-and-place. The real robot
performs each task successfully while replaying demonstrations collected via
AR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language as Polices: Reasoning for Coordinate-Level Embodied
  <span class="highlight-title">Control</span> with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Mikami, Andrew Melnik, Jun Miura, Ville Hautamäki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate experimental results with LLMs that address robotics action
planning problems. Recently, LLMs have been applied in robotics action
planning, particularly using a code generation approach that converts complex
high-level instructions into mid-level policy codes. In contrast, our approach
acquires text descriptions of the task and scene objects, then formulates
action planning through natural language reasoning, and outputs coordinate
level control commands, thus reducing the necessity for intermediate
representation code as policies. Our approach is evaluated on a multi-modal
prompt simulation benchmark, demonstrating that our prompt engineering
experiments with natural language reasoning significantly enhance success rates
compared to its absence. Furthermore, our approach illustrates the potential
for natural language descriptions to transfer robotics skills from known tasks
to previously unseen tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Convex Formulation of Frictional Contact for the Material Point Method
  and Rigid Bodies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeshun Zong, Chenfanfu Jiang, Xuchen Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel convex formulation that seamlessly
integrates the Material Point Method (MPM) with articulated rigid body dynamics
in frictional contact scenarios. We extend the linear corotational hyperelastic
model into the realm of elastoplasticity and include an efficient return
mapping algorithm. This approach is particularly effective for MPM simulations
involving significant deformation and topology changes, while preserving the
convexity of the optimization problem. Our method ensures global convergence,
enabling the use of large simulation time steps without compromising
robustness. We have validated our approach through rigorous testing and
performance evaluations, highlighting its superior capabilities in managing
complex simulations relevant to robotics. Compared to previous MPM based
robotic simulators, our method significantly improves the stability of contact
resolution -- a critical factor in robot manipulation tasks. We make our method
available in the open-source robotics toolkit, Drake.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Certified Human Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadhossein Bahari, Saeed Saadatnejad, Amirhossein Asgari Farsangi, Seyed-Mohsen Moosavi-Dezfooli, Alexandre Alahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction plays an essential role in autonomous vehicles. While
numerous strategies have been developed to enhance the robustness of trajectory
prediction models, these methods are predominantly heuristic and do not offer
guaranteed robustness against adversarial attacks and noisy observations. In
this work, we propose a certification approach tailored for the task of
trajectory prediction. To this end, we address the inherent challenges
associated with trajectory prediction, including unbounded outputs, and
mutli-modality, resulting in a model that provides guaranteed robustness.
Furthermore, we integrate a denoiser into our method to further improve the
performance. Through comprehensive evaluations, we demonstrate the
effectiveness of the proposed technique across various baselines and using
standard trajectory prediction datasets. The code will be made available
online: https://s-attack.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedding Pose Graph, Enabling 3D Foundation Model Capabilities with a
  Compact Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugues Thomas, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the Embedding Pose Graph (EPG), an innovative method that
combines the strengths of foundation models with a simple 3D representation
suitable for robotics applications. Addressing the need for efficient spatial
understanding in robotics, EPG provides a compact yet powerful approach by
attaching foundation model features to the nodes of a pose graph. Unlike
traditional methods that rely on bulky data formats like voxel grids or point
clouds, EPG is lightweight and scalable. It facilitates a range of robotic
tasks, including open-vocabulary querying, disambiguation, image-based
querying, language-directed navigation, and re-localization in 3D environments.
We showcase the effectiveness of EPG in handling these tasks, demonstrating its
capacity to improve how robots interact with and navigate through complex
spaces. Through both qualitative and quantitative assessments, we illustrate
EPG's strong performance and its ability to outperform existing methods in
re-localization. Our work introduces a crucial step forward in enabling robots
to efficiently understand and operate within large-scale 3D spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Projection-free computation of robust <span class="highlight-title">control</span>lable sets with constrained
  zonotopes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abraham P. Vinod, Avishai Weiss, Stefano Di Cairano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of computing robust controllable sets for discrete-time
linear systems with additive uncertainty. We propose a tractable and scalable
approach to inner- and outer-approximate robust controllable sets using
constrained zonotopes, when the additive uncertainty set is a symmetric,
convex, and compact set. Our least-squares-based approach uses novel
closed-form approximations of the Pontryagin difference between a constrained
zonotopic minuend and a symmetric, convex, and compact subtrahend. Unlike
existing approaches, our approach does not rely on convex optimization solvers,
and is projection-free for ellipsoidal and zonotopic uncertainty sets. We also
propose a least-squares-based approach to compute a convex, polyhedral
outer-approximation to constrained zonotopes, and characterize sufficient
conditions under which all these approximations are exact. We demonstrate the
computational efficiency and scalability of our approach in several case
studies, including the design of abort-safe rendezvous trajectories for a
spacecraft in near-rectilinear halo orbit under uncertainty. Our approach can
inner-approximate a 20-step robust controllable set for a 100-dimensional
linear system in under 15 seconds on a standard computer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Reinforcement Learning</span> for Online Testing of Autonomous Driving Systems:
  a Replication and Extension Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Giamattei, Matteo Biagiola, Roberto Pietrantuono, Stefano Russo, Paolo Tonella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a recent study, Reinforcement Learning (RL) used in combination with
many-objective search, has been shown to outperform alternative techniques
(random search and many-objective search) for online testing of Deep Neural
Network-enabled systems. The empirical evaluation of these techniques was
conducted on a state-of-the-art Autonomous Driving System (ADS). This work is a
replication and extension of that empirical study. Our replication shows that
RL does not outperform pure random test generation in a comparison conducted
under the same settings of the original study, but with no confounding factor
coming from the way collisions are measured. Our extension aims at eliminating
some of the possible reasons for the poor performance of RL observed in our
replication: (1) the presence of reward components providing contrasting or
useless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning)
which requires discretization of an intrinsically continuous state space.
Results show that our new RL agent is able to converge to an effective policy
that outperforms random testing. Results also highlight other possible
improvements, which open to further investigations on how to best leverage RL
for online ADS testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with
  Multiple Sensors for Large-Scale Localization and Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhou, Xingxing Li, Shengyu Li, Xuanbin Wang, Shaoquan Feng, Yuxuan Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual simultaneous localization and mapping (VSLAM) has broad applications,
with state-of-the-art methods leveraging deep neural networks for better
robustness and applicability. However, there is a lack of research in fusing
these learning-based methods with multi-sensor information, which could be
indispensable to push related applications to large-scale and complex
scenarios. In this paper, we tightly integrate the trainable deep dense bundle
adjustment (DBA) with multi-sensor information through a factor graph. In the
framework, recurrent optical flow and DBA are performed among sequential
images. The Hessian information derived from DBA is fed into a generic factor
graph for multi-sensor fusion, which employs a sliding window and supports
probabilistic marginalization. A pipeline for visual-inertial integration is
firstly developed, which provides the minimum ability of metric-scale
localization and mapping. Furthermore, other sensors (e.g., global navigation
satellite system) are integrated for driftless and geo-referencing
functionality. Extensive tests are conducted on both public datasets and
self-collected datasets. The results validate the superior localization
performance of our approach, which enables real-time dense mapping in
large-scale environments. The code has been made open-source
(https://github.com/GREAT-WHU/DBA-Fusion).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Matters for Active Texture Recognition With Vision-Based Tactile
  Sensors <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alina Böhm, Tim Schneider, Boris Belousov, Alap Kshirsagar, Lisa Lin, Katja Doerschner, Knut Drewing, Constantin A. Rothkopf, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores active sensing strategies that employ vision-based
tactile sensors for robotic perception and classification of fabric textures.
We formalize the active sampling problem in the context of tactile fabric
recognition and provide an implementation of information-theoretic exploration
strategies based on minimizing predictive entropy and variance of probabilistic
models. Through ablation studies and human experiments, we investigate which
components are crucial for quick and reliable texture recognition. Along with
the active sampling strategies, we evaluate neural network architectures,
representations of uncertainty, influence of data augmentation, and dataset
variability. By evaluating our method on a previously published Active Clothing
Perception Dataset and on a real robotic system, we establish that the choice
of the active exploration strategy has only a minor influence on the
recognition accuracy, whereas data augmentation and dropout rate play a
significantly larger role. In a comparison study, while humans achieve 66.9%
recognition accuracy, our best approach reaches 90.0% in under 5 touches,
highlighting that vision-based tactile sensors are highly effective for fabric
texture recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures, accepted at 2024 IEEE International Conference on
  Robotics and Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loss Regularizing Robotic Terrain Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shakti Deo Kumar, Sudhanshu Tripathi, Krishna Ujjwal, Sarvada Sakshi Jha, Suddhasil De
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locomotion mechanics of legged robots are suitable when pacing through
difficult terrains. Recognising terrains for such robots are important to fully
yoke the versatility of their movements. Consequently, robotic terrain
classification becomes significant to classify terrains in real time with high
accuracy. The conventional classifiers suffer from overfitting problem, low
accuracy problem, high variance problem, and not suitable for live dataset. On
the other hand, classifying a growing dataset is difficult for convolution
based terrain classification. Supervised recurrent models are also not
practical for this classification. Further, the existing recurrent
architectures are still evolving to improve accuracy of terrain classification
based on live variable-length sensory data collected from legged robots. This
paper proposes a new semi-supervised method for terrain classification of
legged robots, avoiding preprocessing of long variable-length dataset. The
proposed method has a stacked Long Short-Term Memory architecture, including a
new loss regularization. The proposed method solves the existing problems and
improves accuracy. Comparison with the existing architectures show the
improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preliminary draft of the work published in IEEE conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhao, Tong Zhang, Zheng Dang, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the relative pose of an object between two images is pivotal to
the success of generalizable object pose estimation. Existing approaches
typically approximate the continuous pose representation with a large number of
discrete pose hypotheses, which incurs a computationally expensive process of
scoring each hypothesis at test time. By contrast, we present a Deep Voxel
Matching Network (DVMNet) that eliminates the need for pose hypotheses and
computes the relative object pose in a single pass. To this end, we map the two
input RGB images, reference and query, to their respective voxelized 3D
representations. We then pass the resulting voxels through a pose estimation
module, where the voxels are aligned and the pose is computed in an end-to-end
fashion by solving a least-squares problem. To enhance robustness, we introduce
a weighted closest voxel algorithm capable of mitigating the impact of noisy
voxels. We conduct extensive experiments on the CO3D, LINEMOD, and Objaverse
datasets, demonstrating that our method delivers more accurate relative pose
estimates for novel objects at a lower computational cost compared to
state-of-the-art methods. Our code is released at:
https://github.com/sailor-z/DVMNet/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reward-Driven Automated Curriculum Learning for Interaction-Aware
  Self-Driving at Unsignalized Intersections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zengqi Peng, Xiao Zhou, Lei Zheng, Yubin Wang, Jun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a reward-driven automated curriculum reinforcement
learning approach for interaction-aware self-driving at unsignalized
intersections, taking into account the uncertainties associated with
surrounding vehicles (SVs). These uncertainties encompass the uncertainty of
SVs' driving intention and also the quantity of SVs. To deal with this problem,
the curriculum set is specifically designed to accommodate a progressively
increasing number of SVs. By implementing an automated curriculum selection
mechanism, the importance weights are rationally allocated across various
curricula, thereby facilitating improved sample efficiency and training
outcomes. Furthermore, the reward function is meticulously designed to guide
the agent towards effective policy exploration. Thus the proposed framework
could proactively address the above uncertainties at unsignalized intersections
by employing the automated curriculum learning technique that progressively
increases task difficulty, and this ensures safe self-driving through effective
interaction with SVs. Comparative experiments are conducted in $Highway\_Env$,
and the results indicate that our approach achieves the highest task success
rate, attains strong robustness to initialization parameters of the curriculum
selection module, and exhibits superior adaptability to diverse situational
configurations at unsignalized intersections. Furthermore, the effectiveness of
the proposed method is validated using the high-fidelity CARLA simulator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaCE-LHMP: Airflow Modelling-Inspired Long-Term Human Motion Prediction
  By Enhancing Laminar Characteristics in Human Flow <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufei Zhu, Han Fan, Andrey Rudenko, Martin Magnusson, Erik Schaffernicht, Achim J. Lilienthal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term human motion prediction (LHMP) is essential for safely operating
autonomous robots and vehicles in populated environments. It is fundamental for
various applications, including motion planning, tracking, human-robot
interaction and safety monitoring. However, accurate prediction of human
trajectories is challenging due to complex factors, including, for example,
social norms and environmental conditions. The influence of such factors can be
captured through Maps of Dynamics (MoDs), which encode spatial motion patterns
learned from (possibly scattered and partial) past observations of motion in
the environment and which can be used for data-efficient, interpretable motion
prediction (MoD-LHMP). To address the limitations of prior work, especially
regarding accuracy and sensitivity to anomalies in long-term prediction, we
propose the Laminar Component Enhanced LHMP approach (LaCE-LHMP). Our approach
is inspired by data-driven airflow modelling, which estimates laminar and
turbulent flow components and uses predominantly the laminar components to make
flow predictions. Based on the hypothesis that human trajectory patterns also
manifest laminar flow (that represents predictable motion) and turbulent flow
components (that reflect more unpredictable and arbitrary motion), LaCE-LHMP
extracts the laminar patterns in human dynamics and uses them for human motion
prediction. We demonstrate the superior prediction performance of LaCE-LHMP
through benchmark comparisons with state-of-the-art LHMP methods, offering an
unconventional perspective and a more intuitive understanding of human movement
patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2024 IEEE International Conference on Robotics and
  Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From One to Many: How Active Robot Swarm Sizes Influence Human Cognitive
  Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Kaduk, Müge Cavdan, Knut Drewing, Heiko Hamann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In robotics, understanding human interaction with autonomous systems is
crucial for enhancing collaborative technologies. We focus on human-swarm
interaction (HSI), exploring how differently sized groups of active robots
affect operators' cognitive and perceptual reactions over different durations.
We analyze the impact of different numbers of active robots within a 15-robot
swarm on operators' time perception, emotional state, flow experience, and task
difficulty perception. Our findings indicate that managing multiple active
robots when compared to one active robot significantly alters time perception
and flow experience, leading to a faster passage of time and increased flow.
More active robots and extended durations cause increased emotional arousal and
perceived task difficulty, highlighting the interaction between robot the
number of active robots and human cognitive processes. These insights inform
the creation of intuitive human-swarm interfaces and aid in developing swarm
robotic systems aligned with human cognitive structures, enhancing human-robot
collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motion Generation from Fine-grained Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunhang Li, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text2motion is to generate motion sequences from given textual
descriptions, where a model should explore the interactions between natural
language instructions and human body movements. While most existing works are
confined to coarse-grained motion descriptions (e.g., "A man squats."),
fine-grained ones specifying movements of relevant body parts are barely
explored. Models trained with coarse texts may not be able to learn mappings
from fine-grained motion-related words to motion primitives, resulting in the
failure in generating motions from unseen descriptions. In this paper, we build
a large-scale language-motion dataset with fine-grained textual descriptions,
FineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we
design a new text2motion model, FineMotionDiffuse, which makes full use of
fine-grained textual information. Our experiments show that FineMotionDiffuse
trained on FineHumanML3D acquires good results in quantitative evaluation. We
also find this model can better generate spatially/chronologically composite
motions by learning the implicit mappings from simple descriptions to the
corresponding basic motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Active-Inactive Obstacle Classification for Time-Optimal
  Collision Avoidance <span class="chip">IROS24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehmetcan Kaymaz, Nazim Kemal Ure
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-optimal obstacle avoidance is a prevalent problem encountered in various
fields, including robotics and autonomous vehicles, where the task involves
determining a path for a moving vehicle to reach its goal while navigating
around obstacles within its environment. This problem becomes increasingly
challenging as the number of obstacles in the environment rises. We propose an
iterative active-inactive obstacle approach, which involves identifying a
subset of the obstacles as "active", that considers solely the effect of the
"active" obstacles on the path of the moving vehicle. The remaining obstacles
are considered "inactive" and are not considered in the path planning process.
The obstacles are classified as 'active' on the basis of previous findings
derived from prior iterations. This approach allows for a more efficient
calculation of the optimal path by reducing the number of obstacles that need
to be considered. The effectiveness of the proposed method is demonstrated with
two different dynamic models using the various number of obstacles. The results
show that the proposed method is able to find the optimal path in a timely
manner, while also being able to handle a large number of obstacles in the
environment and the constraints on the motion of the object.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is under review in IROS24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIPSwarm: Generating Drone Shows from Text Prompts with Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Pueyo, Eduardo Montijano, Ana C. Murillo, Mac Schwager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces CLIPSwarm, a new algorithm designed to automate the
modeling of swarm drone formations based on natural language. The algorithm
begins by enriching a provided word, to compose a text prompt that serves as
input to an iterative approach to find the formation that best matches the
provided word. The algorithm iteratively refines formations of robots to align
with the textual description, employing different steps for "exploration" and
"exploitation". Our framework is currently evaluated on simple formation
targets, limited to contour shapes. A formation is visually represented through
alpha-shape contours and the most representative color is automatically found
for the input word. To measure the similarity between the description and the
visual representation of the formation, we use CLIP [1], encoding text and
images into vectors and assessing their similarity. Subsequently, the algorithm
rearranges the formation to visually represent the word more effectively,
within the given constraints of available drones. Control actions are then
assigned to the drones, ensuring robotic behavior and collision-free movement.
Experimental results demonstrate the system's efficacy in accurately modeling
robot formations from natural language descriptions. The algorithm's
versatility is showcased through the execution of drone shows in photorealistic
simulation with varying shapes. We refer the reader to the supplementary video
for a visual reference of the results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FACT: Fast and Active Coordinate Initialization for Vision-based Drone
  Swarms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Li, Anke Zhao, Yingjian Wang, Ziyi Xu, Xin Zhou, Jinni Zhou, Chao Xu, Fei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Swarm robots have sparked remarkable developments across a range of fields.
While it is necessary for various applications in swarm robots, a fast and
robust coordinate initialization in vision-based drone swarms remains elusive.
To this end, our paper proposes a complete system to recover a swarm's initial
relative pose on platforms with size, weight, and power (SWaP) constraints. To
overcome limited coverage of field-of-view (FoV), the drones rotate in place to
obtain observations. To tackle the anonymous measurements, we formulate a
non-convex rotation estimation problem and transform it into a semi-definite
programming (SDP) problem, which can steadily obtain global optimal values.
Then we utilize the Hungarian algorithm to recover relative translation and
correspondences between observations and drone identities. To safely acquire
complete observations, we actively search for positions and generate feasible
trajectories to avoid collisions. To validate the practicability of our system,
we conduct experiments on a vision-based drone swarm with only stereo cameras
and inertial measurement units (IMUs) as sensors. The results demonstrate that
the system can robustly get accurate relative poses in real time with limited
onboard computation resources. The source code is released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile Robot Localization: a Modular, Odometry-Improving Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Mozzarelli, Luca Cattaneo, Matteo Corno, Sergio Matteo Savaresi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the number of works published in recent years, vehicle localization
remains an open, challenging problem. While map-based localization and SLAM
algorithms are getting better and better, they remain a single point of failure
in typical localization pipelines. This paper proposes a modular localization
architecture that fuses sensor measurements with the outputs of off-the-shelf
localization algorithms. The fusion filter estimates model uncertainties to
improve odometry in case absolute pose measurements are lost entirely. The
architecture is validated experimentally on a real robot navigating
autonomously proving a reduction of the position error of more than 90% with
respect to the odometrical estimate without uncertainty estimation in a
two-minute navigation period without position measurements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE European Control Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast-Poly: A Fast Polyhedral Framework For 3D Multi-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Li, Dedong Liu, Lijun Zhao, Yitao Wu, Xian Wu, Jinghan Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Multi-Object Tracking (MOT) captures stable and comprehensive motion
states of surrounding obstacles, essential for robotic perception. However,
current 3D trackers face issues with accuracy and latency consistency. In this
paper, we propose Fast-Poly, a fast and effective filter-based method for 3D
MOT. Building upon our previous work Poly-MOT, Fast-Poly addresses object
rotational anisotropy in 3D space, enhances local computation densification,
and leverages parallelization technique, improving inference speed and
precision. Fast-Poly is extensively tested on two large-scale tracking
benchmarks with Python implementation. On the nuScenes dataset, Fast-Poly
achieves new state-of-the-art performance with 75.8% AMOTA among all methods
and can run at 34.2 FPS on a personal CPU. On the Waymo dataset, Fast-Poly
exhibits competitive accuracy with 63.6% MOTA and impressive inference speed
(35.5 FPS). The source code is publicly available at
https://github.com/lixiaoyu2000/FastPoly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1st on the NuScenes Tracking benchmark with 75.8 AMOTA and 34.2 FPS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic <span class="highlight-title">Navigation</span> Map Generation for Mobile Robots in Urban
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Mozzarelli, Simone Specchia, Matteo Corno, Sergio Matteo Savaresi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental prerequisite for safe and efficient navigation of mobile robots
is the availability of reliable navigation maps upon which trajectories can be
planned. With the increasing industrial interest in mobile robotics, especially
in urban environments, the process of generating navigation maps has become of
particular interest, being a labor intensive step of the deployment process.
Automating this step is challenging and becomes even more arduous when the
perception capabilities are limited by cost considerations. This paper proposes
an algorithm to automatically generate navigation maps using a typical
navigation-oriented sensor setup: a single top-mounted 3D LiDAR sensor. The
proposed method is designed and validated with the urban environment as the
main use case: it is shown to be able to produce accurate maps featuring
different terrain types, positive obstacles of different heights as well as
negative obstacles. The algorithm is applied to data collected in a typical
urban environment with a wheeled inverted pendulum robot, showing its
robustness against localization, perception and dynamic uncertainties. The
generated map is validated against a human-made map.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Caching-Augmented Lifelong Multi-Agent Path Finding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimin Tang, Zhenghong Yu, Yi Zheng, T. K. Satish Kumar, Jiaoyang Li, Sven Koenig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Path Finding (MAPF), which involves finding collision-free paths
for multiple robots, is crucial in various applications. Lifelong MAPF, where
targets are reassigned to agents as soon as they complete their initial
objectives, offers a more accurate approximation of real-world warehouse
planning. In this paper, we present a novel mechanism named Caching-Augmented
Lifelong MAPF (CAL-MAPF), designed to improve the performance of Lifelong MAPF.
We have developed a new map grid type called cache for temporary item storage
and replacement and designed a lock mechanism for it to improve the stability
of the planning solution. This cache mechanism was evaluated using various
cache replacement policies and a spectrum of input task distributions. We
identified three main factors significantly impacting CAL-MAPF performance
through experimentation: suitable input task distribution, high cache hit rate,
and smooth traffic. Overall, CAL-MAPF has demonstrated potential for
performance improvements in certain task distributions, maps and agent
configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Local and Global Multimodal Features for Place Recognition in
  Aliased and Low-Texture Environments <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto García-Hernández, Riccardo Giubilato, Klaus H. Strobl, Javier Civera, Rudolph Triebel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceptual aliasing and weak textures pose significant challenges to the task
of place recognition, hindering the performance of Simultaneous Localization
and Mapping (SLAM) systems. This paper presents a novel model, called UMF
(standing for Unifying Local and Global Multimodal Features) that 1) leverages
multi-modality by cross-attention blocks between vision and LiDAR features, and
2) includes a re-ranking stage that re-orders based on local feature matching
the top-k candidates retrieved using a global representation. Our experiments,
particularly on sequences captured on a planetary-analogous environment, show
that UMF outperforms significantly previous baselines in those challenging
aliased environments. Since our work aims to enhance the reliability of SLAM in
all situations, we also explore its performance on the widely used RobotCar
dataset, for broader applicability. Code and models are available at
https://github.com/DLR-RM/UMF
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted submission to International Conference on Robotics and
  Automation (ICRA), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Centroidal State Estimation based on the Koopman Embedding for Dynamic
  Legged <span class="highlight-title">Locomotion</span> <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahram Khorshidi, Murad Dawood, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel approach to centroidal state estimation,
which plays a crucial role in predictive model-based control strategies for
dynamic legged locomotion. Our approach uses the Koopman operator theory to
transform the robot's complex nonlinear dynamics into a linear system, by
employing dynamic mode decomposition and deep learning for model construction.
We evaluate both models on their linearization accuracy and capability to
capture both fast and slow dynamic system responses. We then select the most
suitable model for estimation purposes, and integrate it within a moving
horizon estimator. This estimator is formulated as a convex quadratic program,
to facilitate robust, real-time centroidal state estimation. Through extensive
simulation experiments on a quadruped robot executing various dynamic gaits,
our data-driven framework outperforms conventional filtering techniques based
on nonlinear dynamics. Our estimator addresses challenges posed by force/torque
measurement noise in highly dynamic motions and accurately recovers the
centroidal states, demonstrating the adaptability and effectiveness of the
Koopman-based linear representation for complex locomotive behaviors.
Importantly, our model based on dynamic mode decomposition, trained with two
locomotion patterns (trot and jump), successfully estimates the centroidal
states for a different motion (bound) without retraining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation
  in Robotics <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaojun Yu, Ce Hao, Junbo Wang, Wenhai Liu, Liu Liu, Yao Mu, Yang You, Hengxu Yan, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic manipulation in everyday scenarios, especially in unstructured
environments, requires skills in pose-aware object manipulation (POM), which
adapts robots' grasping and handling according to an object's 6D pose.
Recognizing an object's position and orientation is crucial for effective
manipulation. For example, if a mug is lying on its side, it's more effective
to grasp it by the rim rather than the handle. Despite its importance, research
in POM skills remains limited, because learning manipulation skills requires
pose-varying simulation environments and datasets. This paper introduces
ManiPose, a pioneering benchmark designed to advance the study of pose-varying
manipulation tasks. ManiPose encompasses: 1) Simulation environments for POM
feature tasks ranging from 6D pose-specific pick-and-place of single objects to
cluttered scenes, further including interactions with articulated objects. 2) A
comprehensive dataset featuring geometrically consistent and
manipulation-oriented 6D pose labels for 2936 real-world scanned rigid objects
and 100 articulated objects across 59 categories. 3) A baseline for POM,
leveraging the inferencing abilities of LLM (e.g., ChatGPT) to analyze the
relationship between 6D pose and task-specific requirements, offers enhanced
pose-aware grasp prediction and motion planning capabilities. Our benchmark
demonstrates notable advancements in pose estimation, pose-aware manipulation,
and real-robot skill transfer, setting new standards for POM research. We will
open-source the ManiPose benchmark with the final version paper, inviting the
community to engage with our resources, available at our
website:https://sites.google.com/view/manipose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeRM: A Generalist Robotic Model with Mixture-of-experts for <span class="highlight-title">Quadruped</span>
  Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Song, Han Zhao, Pengxiang Ding, Can Cui, Shangke Lyu, Yaning Fan, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task robot learning holds significant importance in tackling diverse
and complex scenarios. However, current approaches are hindered by performance
issues and difficulties in collecting training datasets. In this paper, we
propose GeRM (Generalist Robotic Model). We utilize offline reinforcement
learning to optimize data utilization strategies to learn from both
demonstrations and sub-optimal data, thus surpassing the limitations of human
demonstrations. Thereafter, we employ a transformer-based VLA network to
process multi-modal inputs and output actions. By introducing the
Mixture-of-Experts structure, GeRM allows faster inference speed with higher
whole model capacity, and thus resolves the issue of limited RL parameters,
enhancing model performance in multi-task learning while controlling
computational costs. Through a series of experiments, we demonstrate that GeRM
outperforms other methods across all tasks, while also validating its
efficiency in both training and inference processes. Additionally, we uncover
its potential to acquire emergent skills. Additionally, we contribute the
QUARD-Auto dataset, collected automatically to support our training approach
and foster advancements in multi-task quadruped robot learning. This work
presents a new paradigm for reducing the cost of collecting robot data and
driving progress in the multi-task learning community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MULAN-WC: Multi-Robot Localization Uncertainty-aware Active NeRF with
  Wireless Coordination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiying Wang, Victor Cai, Stephanie Gil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents MULAN-WC, a novel multi-robot 3D reconstruction framework
that leverages wireless signal-based coordination between robots and Neural
Radiance Fields (NeRF). Our approach addresses key challenges in multi-robot 3D
reconstruction, including inter-robot pose estimation, localization uncertainty
quantification, and active best-next-view selection. We introduce a method for
using wireless Angle-of-Arrival (AoA) and ranging measurements to estimate
relative poses between robots, as well as quantifying and incorporating the
uncertainty embedded in the wireless localization of these pose estimates into
the NeRF training loss to mitigate the impact of inaccurate camera poses.
Furthermore, we propose an active view selection approach that accounts for
robot pose uncertainty when determining the next-best views to improve the 3D
reconstruction, enabling faster convergence through intelligent view selection.
Extensive experiments on both synthetic and real-world datasets demonstrate the
effectiveness of our framework in theory and in practice. Leveraging wireless
coordination and localization uncertainty-aware training, MULAN-WC can achieve
high-quality 3d reconstruction which is close to applying the ground truth
camera poses. Furthermore, the quantification of the information gain from a
novel view enables consistent rendering quality improvement with incrementally
captured images by commending the robot the novel view position. Our hardware
experiments showcase the practicality of deploying MULAN-WC to real robotic
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discretizing SO(2)-Equivariant Features for Robotic Kitting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiadong Zhou, Yadan Zeng, Huixu Dong, I-Ming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic kitting has attracted considerable attention in logistics and
industrial settings. However, existing kitting methods encounter challenges
such as low precision and poor efficiency, limiting their widespread
applications. To address these issues, we present a novel kitting framework
that improves both the precision and computational efficiency of complex
kitting tasks. Firstly, our approach introduces a fine-grained orientation
estimation technique in the picking module, significantly enhancing orientation
precision while effectively decoupling computational load from orientation
granularity. This approach combines an SO(2)-equivariant network with a group
discretization operation to preciously predict discrete orientation
distributions. Secondly, we develop the Hand-tool Kitting Dataset (HKD) to
evaluate the performance of different solutions in handling
orientation-sensitive kitting tasks. This dataset comprises a diverse
collection of hand tools and synthetically created kits, which reflects the
complexities encountered in real-world kitting scenarios. Finally, a series of
experiments are conducted to evaluate the performance of the proposed method.
The results demonstrate that our approach offers remarkable precision and
enhanced computational efficiency in robotic kitting tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robotics meets Fluid Dynamics: A Characterization of the Induced Airflow
  around a Quadrotor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Bauersfeld, Koen Muller, Dominic Ziegler, Filippo Coletti, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of quadrotors for diverse applications, from
agriculture to public safety, necessitates an understanding of the aerodynamic
disturbances they create. This paper introduces a computationally lightweight
model for estimating the time-averaged magnitude of the induced flow below
quadrotors in hover. Unlike related approaches that rely on expensive
computational fluid dynamics (CFD) simulations or time-consuming empirical
measurements, our method leverages classical theory from turbulent flows. By
analyzing over 9 hours of flight data from drones of varying sizes within a
large motion capture system, we show that the combined flow from all propellers
of the drone is well-approximated by a turbulent jet. Through the use of a
novel normalization and scaling, we have developed and experimentally validated
a unified model that describes the mean velocity field of the induced flow for
different drone sizes. The model accurately describes the far-field airflow in
a very large volume below the drone which is difficult to simulate in CFD. Our
model, which requires only the drone's mass, propeller size, and drone size for
calculations, offers a practical tool for dynamic planning in multi-agent
scenarios, ensuring safer operations near humans and optimizing sensor
placements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7+1 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Workload Estimation for Unknown Tasks: A <span class="highlight-title">Survey</span> of Machine Learning
  Under Distribution Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josh Bhagat Smith, Julie A. Adams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-robot teams involve humans and robots collaborating to achieve tasks
under various environmental conditions. Successful teaming will require robots
to adapt autonomously to a human teammate's internal state. An important
element of such adaptation is the ability to estimate the human teammates'
workload in unknown situations. Existing workload models use machine learning
to model the relationships between physiological metrics and workload; however,
these methods are susceptible to individual differences and are heavily
influenced by other factors. These methods cannot generalize to unknown tasks,
as they rely on standard machine learning approaches that assume data consists
of independent and identically distributed (IID) samples. This assumption does
not necessarily hold for estimating workload for new tasks. A survey of non-IID
machine learning techniques is presented, where commonly used techniques are
evaluated using three criteria: portability, model complexity, and
adaptability. These criteria are used to argue which techniques are most
applicable for estimating workload for unknown tasks in dynamic, real-time
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Robot Connected Fermat Spiral Coverage <span class="chip">ICAPS24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtao Tang, Hang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Multi-Robot Connected Fermat Spiral (MCFS), a novel
algorithmic framework for Multi-Robot Coverage Path Planning (MCPP) that adapts
Connected Fermat Spiral (CFS) from the computer graphics community to
multi-robot coordination for the first time. MCFS uniquely enables the
orchestration of multiple robots to generate coverage paths that contour around
arbitrarily shaped obstacles, a feature that is notably lacking in traditional
methods. Our framework not only enhances area coverage and optimizes task
performance, particularly in terms of makespan, for workspaces rich in
irregular obstacles but also addresses the challenges of path continuity and
curvature critical for non-holonomic robots by generating smooth paths without
decomposing the workspace. MCFS solves MCPP by constructing a graph of isolines
and transforming MCPP into a combinatorial optimization problem, aiming to
minimize the makespan while covering all vertices. Our contributions include
developing a unified CFS version for scalable and adaptable MCPP, extending it
to MCPP with novel optimization techniques for cost reduction and path
continuity and smoothness, and demonstrating through extensive experiments that
MCFS outperforms existing MCPP methods in makespan, path curvature, coverage
ratio, and overlapping ratio. Our research marks a significant step in MCPP,
showcasing the fusion of computer graphics and automated planning principles to
advance the capabilities of multi-robot systems in complex environments. Our
code is available at https://github.com/reso1/MCFS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ICAPS24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ POLICEd RL: Learning Closed-Loop Robot <span class="highlight-title">Control</span> Policies with Provable
  Satisfaction of Hard Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Baptiste Bouvier, Kartik Nagpal, Negar Mehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we seek to learn a robot policy guaranteed to satisfy state
constraints. To encourage constraint satisfaction, existing RL algorithms
typically rely on Constrained Markov Decision Processes and discourage
constraint violations through reward shaping. However, such soft constraints
cannot offer verifiable safety guarantees. To address this gap, we propose
POLICEd RL, a novel RL algorithm explicitly designed to enforce affine hard
constraints in closed-loop with a black-box environment. Our key insight is to
force the learned policy to be affine around the unsafe set and use this affine
region as a repulsive buffer to prevent trajectories from violating the
constraint. We prove that such policies exist and guarantee constraint
satisfaction. Our proposed framework is applicable to both systems with
continuous and discrete state and action spaces and is agnostic to the choice
of the RL training algorithm. Our results demonstrate the capacity of POLICEd
RL to enforce hard constraints in robotic tasks while significantly
outperforming existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Map-Aware Human Pose Prediction for Robot Follow-Ahead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyuan Jiang, Burak Susam, Jun-Jee Chao, Volkan Isler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the robot follow-ahead task, a mobile robot is tasked to maintain its
relative position in front of a moving human actor while keeping the actor in
sight. To accomplish this task, it is important that the robot understand the
full 3D pose of the human (since the head orientation can be different than the
torso) and predict future human poses so as to plan accordingly. This
prediction task is especially tricky in a complex environment with junctions
and multiple corridors. In this work, we address the problem of forecasting the
full 3D trajectory of a human in such environments. Our main insight is to show
that one can first predict the 2D trajectory and then estimate the full 3D
trajectory by conditioning the estimator on the predicted 2D trajectory. With
this approach, we achieve results comparable or better than the
state-of-the-art methods three times faster. As part of our contribution, we
present a new dataset where, in contrast to existing datasets, the human motion
is in a much larger area than a single room. We also present a complete robot
system that integrates our human pose forecasting network on the mobile robot
to enable real-time robot follow-ahead and present results from real-world
experiments in multiple buildings on campus. Our project page, including
supplementary material and videos, can be found at:
https://qingyuan-jiang.github.io/iros2024_poseForecasting/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Look Before You Leap: Socially Acceptable High-Speed Ground Robot
  <span class="highlight-title">Navigation</span> in Crowded Hallways <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lakshay Sharma, Jonathan P. How
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To operate safely and efficiently, autonomous warehouse/delivery robots must
be able to accomplish tasks while navigating in dynamic environments and
handling the large uncertainties associated with the motions/behaviors of other
robots and/or humans. A key scenario in such environments is the hallway
problem, where robots must operate in the same narrow corridor as human traffic
going in one or both directions. Traditionally, robot planners have tended to
focus on socially acceptable behavior in the hallway scenario at the expense of
performance. This paper proposes a planner that aims to address the consequent
"robot freezing problem" in hallways by allowing for "peek-and-pass" maneuvers.
We then go on to demonstrate in simulation how this planner improves robot time
to goal without violating social norms. Finally, we show initial hardware
demonstrations of this planner in the real world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Waypoint-Based <span class="highlight-title">Reinforcement Learning</span> for Robot Manipulation Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaunak A. Mehta, Soheil Habibian, Dylan P. Losey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot arms should be able to learn new tasks. One framework here is
reinforcement learning, where the robot is given a reward function that encodes
the task, and the robot autonomously learns actions to maximize its reward.
Existing approaches to reinforcement learning often frame this problem as a
Markov decision process, and learn a policy (or a hierarchy of policies) to
complete the task. These policies reason over hundreds of fine-grained actions
that the robot arm needs to take: e.g., moving slightly to the right or
rotating the end-effector a few degrees. But the manipulation tasks that we
want robots to perform can often be broken down into a small number of
high-level motions: e.g., reaching an object or turning a handle. In this paper
we therefore propose a waypoint-based approach for model-free reinforcement
learning. Instead of learning a low-level policy, the robot now learns a
trajectory of waypoints, and then interpolates between those waypoints using
existing controllers. Our key novelty is framing this waypoint-based setting as
a sequence of multi-armed bandits: each bandit problem corresponds to one
waypoint along the robot's motion. We theoretically show that an ideal solution
to this reformulation has lower regret bounds than standard frameworks. We also
introduce an approximate posterior sampling solution that builds the robot's
motion one waypoint at a time. Results across benchmark simulations and two
real-world experiments suggest that this proposed approach learns new tasks
more quickly than state-of-the-art baselines. See videos here:
https://youtu.be/MMEd-lYfq4Y
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UNO Push: Unified Nonprehensile Object Pushing via Non-Parametric
  Estimation and Model Predictive <span class="highlight-title">Control</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaotian Wang, Kejia Ren, Kaiyu Hang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonprehensile manipulation through precise pushing is an essential skill that
has been commonly challenged by perception and physical uncertainties, such as
those associated with contacts, object geometries, and physical properties. For
this, we propose a unified framework that jointly addresses system modeling,
action generation, and control. While most existing approaches either heavily
rely on a priori system information for analytic modeling, or leverage a large
dataset to learn dynamic models, our framework approximates a system transition
function via non-parametric learning only using a small number of exploratory
actions (ca. 10). The approximated function is then integrated with model
predictive control to provide precise pushing manipulation. Furthermore, we
show that the approximated system transition functions can be robustly
transferred across novel objects while being online updated to continuously
improve the manipulation accuracy. Through extensive experiments on a real
robot platform with a set of novel objects and comparing against a
state-of-the-art baseline, we show that the proposed unified framework is a
light-weight and highly effective approach to enable precise pushing
manipulation all by itself. Our evaluation results illustrate that the system
can robustly ensure millimeter-level precision and can straightforwardly work
on any novel object.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Security in Multi-Robot Systems through Co-Observation
  Planning, Reachability Analysis, and Network Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Yang, Roberto Tron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses security challenges in multi-robot systems (MRS) where
adversaries may compromise robot control, risking unauthorized access to
forbidden areas. We propose a novel multi-robot optimal planning algorithm that
integrates mutual observations and introduces reachability constraints for
enhanced security. This ensures that, even with adversarial movements,
compromised robots cannot breach forbidden regions without missing scheduled
co-observations. The reachability constraint uses ellipsoidal
over-approximation for efficient intersection checking and gradient
computation. To enhance system resilience and tackle feasibility challenges, we
also introduce sub-teams. These cohesive units replace individual robot
assignments along each route, enabling redundant robots to deviate for
co-observations across different trajectories, securing multiple sub-teams
without requiring modifications. We formulate the cross-trajectory
co-observation plan by solving a network flow coverage problem on the
checkpoint graph generated from the original unsecured MRS trajectories,
providing the same security guarantees against plan-deviation attacks. We
demonstrate the effectiveness and robustness of our proposed algorithm, which
significantly strengthens the security of multi-robot systems in the face of
adversarial threats.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, submitted to IEEE Transactions on Control of
  Network Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Rule-Compliance Path Planner for Lane-Merge Scenarios Based on
  Responsibility-Sensitive Safety <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Lin, Ehsan Javanmardi, Yuze Jiang, Manabu Tsukada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lane merging is one of the critical tasks for self-driving cars, and how to
perform lane-merge maneuvers effectively and safely has become one of the
important standards in measuring the capability of autonomous driving systems.
However, due to the ambiguity in driving intentions and right-of-way issues,
the lane merging process in autonomous driving remains deficient in terms of
maintaining or ceding the right-of-way and attributing liability, which could
result in protracted durations for merging and problems such as trajectory
oscillation. Hence, we present a rule-compliance path planner (RCPP) for
lane-merge scenarios, which initially employs the extended
responsibility-sensitive safety (RSS) to elucidate the right-of-way, followed
by the potential field-based sigmoid planner for path generation. In the
simulation, we have validated the efficacy of the proposed algorithm. The
algorithm demonstrated superior performance over previous approaches in aspects
such as merging time (Saved 72.3%), path length (reduced 53.4%), and
eliminating the trajectory oscillation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated <span class="highlight-title">reinforcement learning</span> for robot motion planning with
  zero-shot generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyuan Yuan, Siyuan Xu, Minghui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the problem of learning a control policy for robot
motion planning with zero-shot generalization, i.e., no data collection and
policy adaptation is needed when the learned policy is deployed in new
environments. We develop a federated reinforcement learning framework that
enables collaborative learning of multiple learners and a central server, i.e.,
the Cloud, without sharing their raw data. In each iteration, each learner
uploads its local control policy and the corresponding estimated normalized
arrival time to the Cloud, which then computes the global optimum among the
learners and broadcasts the optimal policy to the learners. Each learner then
selects between its local control policy and that from the Cloud for next
iteration. The proposed framework leverages on the derived zero-shot
generalization guarantees on arrival time and safety. Theoretical guarantees on
almost-sure convergence, almost consensus, Pareto improvement and optimality
gap are also provided. Monte Carlo simulation is conducted to evaluate the
proposed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMCO: Adaptive Multimodal Coupling of Vision and Proprioception for
  <span class="highlight-title">Quadruped</span> Robot <span class="highlight-title">Navigation</span> in Outdoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Elnoor, Kasun Weerakoon, Adarsh Jagan Sathyamoorthy, Tianrui Guan, Vignesh Rajagopal, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present AMCO, a novel navigation method for quadruped robots that
adaptively combines vision-based and proprioception-based perception
capabilities. Our approach uses three cost maps: general knowledge map;
traversability history map; and current proprioception map; which are derived
from a robot's vision and proprioception data, and couples them to obtain a
coupled traversability cost map for navigation. The general knowledge map
encodes terrains semantically segmented from visual sensing, and represents a
terrain's typically expected traversability. The traversability history map
encodes the robot's recent proprioceptive measurements on a terrain and its
semantic segmentation as a cost map. Further, the robot's present
proprioceptive measurement is encoded as a cost map in the current
proprioception map. As the general knowledge map and traversability history map
rely on semantic segmentation, we evaluate the reliability of the visual
sensory data by estimating the brightness and motion blur of input RGB images
and accordingly combine the three cost maps to obtain the coupled
traversability cost map used for navigation. Leveraging this adaptive coupling,
the robot can depend on the most reliable input modality available. Finally, we
present a novel planner that selects appropriate gaits and velocities for
traversing challenging outdoor environments using the coupled traversability
cost map. We demonstrate AMCO's navigation performance in different real-world
outdoor environments and observe 10.8%-34.9% reduction w.r.t. two stability
metrics, and up to 50% improvement in terms of success rate compared to current
navigation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Contact Model based on Denoising Diffusion to Learn Variable Impedance
  <span class="highlight-title">Control</span> for Contact-rich Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masashi Okada, Mayumi Komatsu, Tadahiro Taniguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a novel approach is proposed for learning robot control in
contact-rich tasks such as wiping, by developing Diffusion Contact Model (DCM).
Previous methods of learning such tasks relied on impedance control with
time-varying stiffness tuning by performing Bayesian optimization by
trial-and-error with robots. The proposed approach aims to reduce the cost of
robot operation by predicting the robot contact trajectories from the variable
stiffness inputs and using neural models. However, contact dynamics are
inherently highly nonlinear, and their simulation requires iterative
computations such as convex optimization. Moreover, approximating such
computations by using finite-layer neural models is difficult. To overcome
these limitations, the proposed DCM used the denoising diffusion models that
could simulate the complex dynamics via iterative computations of multi-step
denoising, thus improving the prediction accuracy. Stiffness tuning experiments
conducted in simulated and real environments showed that the DCM achieved
comparable performance to a conventional robot-based optimization method while
reducing the number of robot trials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Designing Library of Skill-Agents for Hardware-Level Reusability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Takamatsu, Daichi Saito, Katsushi Ikeuchi, Atsushi Kanehira, Kazuhiro Sasabuchi, Naoki Wake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To use new robot hardware in a new environment, it is necessary to develop a
control program tailored to that specific robot in that environment.
Considering the reusability of software among robots is crucial to minimize the
effort involved in this process and maximize software reuse across different
robots in different environments. This paper proposes a method to remedy this
process by considering hardware-level reusability, using
Learning-from-observation (LfO) paradigm with a pre-designed skill-agent
library. The LfO framework represents the required actions in
hardware-independent representations, referred to as task models, from
observing human demonstrations, capturing the necessary parameters for the
interaction between the environment and the robot. When executing the desired
actions from the task models, a set of skill agents is employed to convert the
representations into robot commands. This paper focuses on the latter part of
the LfO framework, utilizing the set to generate robot actions from the task
models, and explores a hardware-independent design approach for these skill
agents. These skill agents are described in a hardware-independent manner,
considering the relative relationship between the robot's hand position and the
environment. As a result, it is possible to execute these actions on robots
with different hardware configurations by simply swapping the inverse
kinematics solver. This paper, first, defines a necessary and sufficient
skill-agent set corresponding to cover all possible actions, and considers the
design principles for these skill agents in the library. We provide concrete
examples of such skill agents and demonstrate the practicality of using these
skill agents by showing that the same representations can be executed on two
different robots, Nextage and Fetch, using the proposed skill-agents set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Working Backwards: Learning to Place by Picking <span class="chip">IROS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Limoyo, Abhisek Konar, Trevor Ablett, Jonathan Kelly, Francois R. Hogan, Gregory Dudek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present placing via picking (PvP), a method to autonomously collect
real-world demonstrations for a family of placing tasks in which objects must
be manipulated to specific contact-constrained locations. With PvP, we approach
the collection of robotic object placement demonstrations by reversing the
grasping process and exploiting the inherent symmetry of the pick and place
problems. Specifically, we obtain placing demonstrations from a set of grasp
sequences of objects initially located at their target placement locations. Our
system can collect hundreds of demonstrations in contact-constrained
environments without human intervention by combining two modules: tactile
regrasping and compliant control for grasps. We train a policy directly from
visual observations through behavioral cloning, using the
autonomously-collected demonstrations. By doing so, the policy can generalize
to object placement scenarios outside of the training environment without
privileged information (e.g., placing a plate picked up from a table). We
validate our approach in home robotic scenarios that include dishwasher loading
and table setting. Our approach yields robotic placing policies that outperform
policies trained with kinesthetic teaching, both in terms of performance and
data efficiency, while requiring no human supervision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE/RSJ International Conference on Intelligent
  Robotics and Systems (IROS'24), Abu Dhabi, UAE, Oct 14-18, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhotoBot: Reference-Guided Interactive Photography via Natural Language <span class="chip">IROS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Limoyo, Jimmy Li, Dmitriy Rivkin, Jonathan Kelly, Gregory Dudek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PhotoBot, a framework for fully automated photo acquisition
based on an interplay between high-level human language guidance and a robot
photographer. We propose to communicate photography suggestions to the user via
reference images that are selected from a curated gallery. We leverage a visual
language model (VLM) and an object detector to characterize the reference
images via textual descriptions and then use a large language model (LLM) to
retrieve relevant reference images based on a user's language query through
text-based reasoning. To correspond the reference image and the observed scene,
we exploit pre-trained features from a vision transformer capable of capturing
semantic similarity across marked appearance variations. Using these features,
we compute pose adjustments for an RGB-D camera by solving a
perspective-n-point (PnP) problem. We demonstrate our approach using a
manipulator equipped with a wrist camera. Our user studies show that photos
taken by PhotoBot are often more aesthetically pleasing than those taken by
users themselves, as measured by human feedback. We also show that PhotoBot can
generalize to other reference sources such as paintings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE/RSJ International Conference on Intelligent
  Robotics and Systems (IROS'24), Abu Dhabi, UAE, Oct 14-18, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Social Robots for Sleep Health: A Scoping <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Nikhil Antony, Mengchi Li, Shu-Han Lin, Junxin Li, Chien-Ming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Poor sleep health is an increasingly concerning public healthcare crisis,
especially when coupled with a dwindling number of health professionals
qualified to combat it. However, there is a growing body of scientific
literature on the use of digital technologies in supporting and sustaining
individuals' healthy sleep habits. Social robots are a relatively recent
technology that has been used to facilitate health care interventions and may
have potential in improving sleep health outcomes, as well. Social robots'
unique characteristics -- such as anthropomorphic physical embodiment or
effective communication methods -- help to engage users and motivate them to
comply with specific interventions, thus improving the interventions' outcomes.
This scoping review aims to evaluate current scientific evidence for employing
social robots in sleep health interventions, identify critical research gaps,
and suggest future directions for developing and using social robots to improve
people's sleep health. Our analysis of the reviewed studies found them limited
due to a singular focus on the older adult population, use of small sample
sizes, limited intervention durations, and other compounding factors.
Nevertheless, the reviewed studies reported several positive outcomes,
highlighting the potential social robots hold in this field. Although our
review found limited clinical evidence for the efficacy of social robots as
purveyors of sleep health interventions, it did elucidate the potential for a
successful future in this domain if current limitations are addressed and more
research is conducted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic
  Object Rearrangement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15821v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15821v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Chang, Kai Gao, Kowndinya Boyalakuntla, Alex Lee, Baichuan Huang, Harish Udhaya Kumar, Jinjin Yu, Abdeslam Boularias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel approach to the executable semantic object rearrangement
problem. In this challenge, a robot seeks to create an actionable plan that
rearranges objects within a scene according to a pattern dictated by a natural
language description. Unlike existing methods such as StructFormer and
StructDiffusion, which tackle the issue in two steps by first generating poses
and then leveraging a task planner for action plan formulation, our method
concurrently addresses pose generation and action planning. We achieve this
integration using a Language-Guided Monte-Carlo Tree Search (LGMCTS).
Quantitative evaluations are provided on two simulation datasets, and
complemented by qualitative tests with a real robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code and supplementary materials are accessible at
  https://github.com/changhaonan/LG-MCTS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy
  Learning for Robotic <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06192v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06192v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhrij Patel, Kasun Weerakoon, Wesley A. Suttle, Alec Koppel, Brian M. Sadler, Tianyi Zhou, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory length stands as a crucial hyperparameter within reinforcement
learning (RL) algorithms, significantly contributing to the sample inefficiency
in robotics applications. Motivated by the pivotal role trajectory length plays
in the training process, we introduce Ada-NAV, a novel adaptive trajectory
length scheme designed to enhance the training sample efficiency of RL
algorithms in robotic navigation tasks. Unlike traditional approaches that
treat trajectory length as a fixed hyperparameter, we propose to dynamically
adjust it based on the entropy of the underlying navigation policy.
Interestingly, Ada-NAV can be applied to both existing on-policy and off-policy
RL methods, which we demonstrate by empirically validating its efficacy on
three popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and
Soft Actor-Critic (SAC). We demonstrate through simulated and real-world
robotic experiments that Ada-NAV outperforms conventional methods that employ
constant or randomly sampled trajectory lengths. Specifically, for a fixed
sample budget, Ada-NAV achieves an 18\% increase in navigation success rate, a
20-38\% reduction in navigation path length, and a 9.32\% decrease in elevation
costs. Furthermore, we showcase the versatility of Ada-NAV by integrating it
with the Clearpath Husky robot, illustrating its applicability in complex
outdoor environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception
  Uncertainties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kshitij Sirohi, Daniel Büscher, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of a robust map-based localization system is essential for
the operation of many autonomously navigating vehicles. Since uncertainty is an
inevitable part of perception, it is beneficial for the robustness of the robot
to consider it in typical downstream tasks of navigation stacks. In particular
localization and mapping methods, which in modern systems often employ
convolutional neural networks (CNNs) for perception tasks, require proper
uncertainty estimates. In this work, we present uncertainty-aware Panoptic
Localization and Mapping (uPLAM), which employs pixel-wise uncertainty
estimates for panoptic CNNs as a bridge to fuse modern perception with
classical probabilistic localization and mapping approaches. Beyond the
perception, we introduce an uncertainty-based map aggregation technique to
create accurate panoptic maps, containing surface semantics and landmark
instances. Moreover, we provide cell-wise map uncertainties, and present a
particle filter-based localization method that employs perception
uncertainties. Extensive evaluations show that our proposed incorporation of
uncertainties leads to more accurate maps with reliable uncertainty estimates
and improved localization accuracy. Additionally, we present the Freiburg
Panoptic Driving dataset for evaluating panoptic mapping and localization
methods. We make our code and dataset available at:
\url{http://uplam.cs.uni-freiburg.de}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exposing the Unseen: Exposure Time Emulation for Offline Benchmarking of
  Vision Algorithms <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13139v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13139v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Gamache, Jean-Michel Fortin, Matěj Boxan, Maxime Vaidis, François Pomerleau, Philippe Giguère
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Odometry (VO) is one of the fundamental tasks in computer vision for
robotics. However, its performance is deeply affected by High Dynamic Range
(HDR) scenes, omnipresent outdoor. While new Automatic-Exposure (AE) approaches
to mitigate this have appeared, their comparison in a reproducible manner is
problematic. This stems from the fact that the behavior of AE depends on the
environment, and it affects the image acquisition process. Consequently, AE has
traditionally only been benchmarked in an online manner, making the experiments
non-reproducible. To solve this, we propose a new methodology based on an
emulator that can generate images at any exposure time. It leverages BorealHDR,
a unique multi-exposure stereo dataset collected over 10 km, on 55 trajectories
with challenging illumination conditions. Moreover, it includes
lidar-inertial-based global maps with pose estimation for each image frame as
well as Global Navigation Satellite System (GNSS) data, for comparison. We show
that using these images acquired at different exposure times, we can emulate
realistic images, keeping a Root-Mean-Square Error (RMSE) below 1.78 % compared
to ground truth images. To demonstrate the practicality of our approach for
offline benchmarking, we compared three state-of-the-art AE algorithms on key
elements of Visual Simultaneous Localization And Mapping (VSLAM) pipeline,
against four baselines. Consequently, reproducible evaluation of AE is now
possible, speeding up the development of future approaches. Our code and
dataset are available online at this link:
https://github.com/norlab-ulaval/BorealHDR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigation of Enhanced Inertial <span class="highlight-title">Navigation</span> Algorithms by Functional
  Iteration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05855v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05855v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyan Jiang, Maoran Zhu, Yuanxin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The defects of the traditional strapdown inertial navigation algorithms
become well acknowledged and the corresponding enhanced algorithms have been
quite recently proposed trying to mitigate both theoretical and algorithmic
defects. In this paper, the analytical accuracy evaluation of both the
traditional algorithms and the enhanced algorithms is investigated, against the
true reference for the first time enabled by the functional iteration approach
having provable convergence. The analyses by the help of MATLAB Symbolic
Toolbox show that the resultant error orders of all algorithms under
investigation are consistent with those in the existing literatures, and the
enhanced attitude algorithm notably reduces error orders of the traditional
counterpart, while the impact of the enhanced velocity algorithm on error order
reduction is insignificant. Simulation results agree with analyses that the
superiority of the enhanced algorithm over the traditional one in the
body-frame attitude computation scenario diminishes significantly in the entire
inertial navigation computation scenario, while the functional iteration
approach possesses significant accuracy superiority even under sustained lowly
dynamic conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intention-Aware Planner for Robust and Safe Aerial Tracking <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuyu Ren, Huan Yu, Jiajun Dai, Zhi Zheng, Jun Meng, Li Xu, Chao Xu, Fei Gao, Yanjun Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous target tracking with quadrotors has wide applications in many
scenarios, such as cinematographic follow-up shooting or suspect chasing.
Target motion prediction is necessary when designing the tracking planner.
However, the widely used constant velocity or constant rotation assumption can
not fully capture the dynamics of the target. The tracker may fail when the
target happens to move aggressively, such as sudden turn or deceleration. In
this paper, we propose an intention-aware planner by additionally considering
the intention of the target to enhance safety and robustness in aerial tracking
applications. Firstly, a designated intention prediction method is proposed,
which combines a user-defined potential assessment function and a state
observation function. A reachable region is generated to specifically evaluate
the turning intentions. Then we design an intention-driven hybrid A* method to
predict the future possible positions for the target. Finally, an
intention-aware optimization approach is designed to generate a
spatial-temporal optimal trajectory, allowing the tracker to perceive
unexpected situations from the target. Benchmark comparisons and real-world
experiments are conducted to validate the performance of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surfer: Progressive Reasoning with World Models for Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11335v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11335v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengzhen Ren, Kaidong Zhang, Hetao Zheng, Zixuan Li, Yuhang Wen, Fengda Zhu, Mas Ma, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering how to make the model accurately understand and follow natural
language instructions and perform actions consistent with world knowledge is a
key challenge in robot manipulation. This mainly includes human fuzzy
instruction reasoning and the following of physical knowledge. Therefore, the
embodied intelligence agent must have the ability to model world knowledge from
training data. However, most existing vision and language robot manipulation
methods mainly operate in less realistic simulator and language settings and
lack explicit modeling of world knowledge. To bridge this gap, we introduce a
novel and simple robot manipulation framework, called Surfer. It is based on
the world model, treats robot manipulation as a state transfer of the visual
scene, and decouples it into two parts: action and scene. Then, the
generalization ability of the model on new instructions and new scenes is
enhanced by explicit modeling of the action and scene prediction in multi-modal
information. In addition to the framework, we also built a robot manipulation
simulator that supports full physics execution based on the MuJoCo physics
engine. It can automatically generate demonstration training data and test
data, effectively reducing labor costs. To conduct a comprehensive and
systematic evaluation of the robot manipulation model in terms of language
understanding and physical execution, we also created a robotic manipulation
benchmark with progressive reasoning tasks, called SeaWave. It contains 4
levels of progressive reasoning tasks and can provide a standardized testing
platform for embedded AI agents in multi-modal environments. On average, Surfer
achieved a success rate of 54.74% on the defined four levels of manipulation
tasks, exceeding the best baseline performance of 47.64%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM3:Large Language Model-based Task and Motion Planning with Motion
  Failure Reasoning <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11552v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11552v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu Zhang, Ying Nian Wu, Song-Chun Zhu, Hangxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional Task and Motion Planning (TAMP) approaches rely on manually
crafted interfaces connecting symbolic task planning with continuous motion
generation. These domain-specific and labor-intensive modules are limited in
addressing emerging tasks in real-world settings. Here, we present LLM^3, a
novel Large Language Model (LLM)-based TAMP framework featuring a
domain-independent interface. Specifically, we leverage the powerful reasoning
and planning capabilities of pre-trained LLMs to propose symbolic action
sequences and select continuous action parameters for motion planning.
Crucially, LLM^3 incorporates motion planning feedback through prompting,
allowing the LLM to iteratively refine its proposals by reasoning about motion
failure. Consequently, LLM^3 interfaces between task planning and motion
planning, alleviating the intricate design process of handling domain-specific
messages between them. Through a series of simulations in a box-packing domain,
we quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP
problems and the efficiency in selecting action parameters. Ablation studies
underscore the significant contribution of motion failure reasoning to the
success of LLM^3. Furthermore, we conduct qualitative experiments on a physical
manipulator, demonstrating the practical applicability of our approach in
real-world settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024. Codes available:
  https://github.com/AssassinWS/LLM-TAMP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pseudo-rigid body networks: learning interpretable deformable object
  dynamics from partial observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07975v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07975v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shamil Mamedov, A. René Geist, Jan Swevers, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate prediction of deformable linear object (DLO) dynamics is challenging
if the task at hand requires a human-interpretable yet computationally fast
model. In this work, we draw inspiration from the pseudo-rigid body method
(PRB) and model a DLO as a serial chain of rigid bodies whose internal state is
unrolled through time by a dynamics network. This dynamics network is trained
jointly with a physics-informed encoder which maps observed motion variables to
the DLO's hidden state. To encourage that the state acquires a physically
meaningful representation, we leverage the forward kinematics of the PRB model
as decoder. We demonstrate in robot experiments that the proposed DLO dynamics
model provides physically interpretable predictions from partial observations
while being on par with black-box models regarding prediction accuracy. The
project code is available at: http://tinyurl.com/prb-networks
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Camera Height Doesn't Change: Unsupervised Training for Metric Monocular
  Road-Scene Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genki Kinoshita, Ko Nishino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel training method for making any monocular
depth network learn absolute scale and estimate metric road-scene depth just
from regular training data, i.e., driving videos. We refer to this training
framework as StableCamH. The key idea is to leverage cars found on the road as
sources of scale supervision but to incorporate them in the training robustly.
StableCamH detects and estimates the sizes of cars in the frame and aggregates
scale information extracted from them into a camera height estimate whose
consistency across the entire video sequence is enforced as scale supervision.
This realizes robust unsupervised training of any, otherwise scale-oblivious,
monocular depth network to become not only scale-aware but also metric-accurate
without the need for auxiliary sensors and extra supervision. Extensive
experiments on the KITTI and Cityscapes datasets show the effectiveness of
StableCamH and its state-of-the-art accuracy compared with related methods. We
also show that StableCamH enables training on mixed datasets of different
camera heights, which leads to larger-scale training and thus higher
generalization. Metric depth reconstruction is essential in any road-scene
visual modeling, and StableCamH democratizes its deployment by establishing the
means to train any model as a metric depth estimator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NDT-Map-Code: A 3D global descriptor for real-time loop closure
  detection in lidar SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08221v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08221v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhou Liao, Wenlei Yan, Li Sun, Xinhui Bai, Zhenxing You, Hongyuan Yuan, Chunyun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Loop-closure detection, also known as place recognition, aiming to identify
previously visited locations, is an essential component of a SLAM system.
Existing research on lidar-based loop closure heavily relies on dense point
cloud and 360 FOV lidars. This paper proposes an out-of-the-box NDT (Normal
Distribution Transform) based global descriptor, NDT-Map-Code, designed for
both on-road driving and underground valet parking scenarios. NDT-Map-Code can
be directly extracted from the NDT map without the need for a dense point
cloud, resulting in excellent scalability and low maintenance cost. The NDT
representation is leveraged to identify representative patterns, which are
further encoded according to their spatial location (bearing, range, and
height). Experimental results on the NIO underground parking lot dataset and
the KITTI dataset demonstrate that our method achieves significantly better
performance compared to the state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-State Fusion: Improving Deep Neural Networks for Autonomous
  Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06112v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06112v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Cereda, Stefano Bonato, Mirko Nava, Alessandro Giusti, Daniele Palossi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based deep learning perception fulfills a paramount role in robotics,
facilitating solutions to many challenging scenarios, such as acrobatic
maneuvers of autonomous unmanned aerial vehicles (UAVs) and robot-assisted
high-precision surgery. Control-oriented end-to-end perception approaches,
which directly output control variables for the robot, commonly take advantage
of the robot's state estimation as an auxiliary input. When intermediate
outputs are estimated and fed to a lower-level controller, i.e. mediated
approaches, the robot's state is commonly used as an input only for egocentric
tasks, which estimate physical properties of the robot itself. In this work, we
propose to apply a similar approach for the first time -- to the best of our
knowledge -- to non-egocentric mediated tasks, where the estimated outputs
refer to an external subject. We prove how our general methodology improves the
regression performance of deep convolutional neural networks (CNNs) on a broad
class of non-egocentric 3D pose estimation problems, with minimal computational
cost. By analyzing three highly-different use cases, spanning from grasping
with a robotic arm to following a human subject with a pocket-sized UAV, our
results consistently improve the R\textsuperscript{2} regression metric, up to
+0.51, compared to their stateless baselines. Finally, we validate the in-field
performance of a closed-loop autonomous cm-scale UAV on the human pose
estimation task. Our results show a significant reduction, i.e., 24\% on
average, on the mean absolute error of our stateful CNN, compared to a
State-of-the-Art stateless counterpart.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication in the Journal of
  Intelligent & Robotic Systems. \copyright 2024 Springer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Pose-graph <span class="highlight-title">Optimization</span> with Multi-level Partitioning for
  Collaborative SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01657v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01657v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cunhao Li, Peng Yi, Guanghui Guo, Yiguang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The back-end module of Distributed Collaborative Simultaneous Localization
and Mapping (DCSLAM) requires solving a nonlinear Pose Graph Optimization (PGO)
under a distributed setting, also known as SE(d)-synchronization. Most existing
distributed graph optimization algorithms employ a simple sequential
partitioning scheme, which may result in unbalanced subgraph dimensions due to
the different geographic locations of each robot, and hence imposes extra
communication load. Moreover, the performance of current Riemannian
optimization algorithms can be further accelerated. In this letter, we propose
a novel distributed pose graph optimization algorithm combining multi-level
partitioning with an accelerated Riemannian optimization method. Firstly, we
employ the multi-level graph partitioning algorithm to preprocess the naive
pose graph to formulate a balanced optimization problem. In addition, inspired
by the accelerated coordinate descent method, we devise an Improved Riemannian
Block Coordinate Descent (IRBCD) algorithm and the critical point obtained is
globally optimal. Finally, we evaluate the effects of four common graph
partitioning approaches on the correlation of the inter-subgraphs, and discover
that the Highest scheme has the best partitioning performance. Also, we
implement simulations to quantitatively demonstrate that our proposed algorithm
outperforms the state-of-the-art distributed pose graph optimization protocols.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Results and Lessons Learned from Autonomous Driving Transportation
  Services in Airfield, Crowded Indoor, and Urban Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Doosan Baek, Sanghyun Kim, Seung-Woo Seo, Sang-Hyun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles have been actively investigated over the past few
decades. Several recent works show the potential of autonomous vehicles in
urban environments with impressive experimental results. However, these works
note that autonomous vehicles are still occasionally inferior to expert drivers
in complex scenarios. Furthermore, they do not focus on the possibilities of
autonomous driving transportation services in other areas beyond urban
environments. This paper presents the research results and lessons learned from
autonomous driving transportation services in airfield, crowded indoor, and
urban environments. We discuss how we address several unique challenges in
these diverse environments. We also offer an overview of remaining challenges
that have not received much attention but must be addressed. This paper aims to
share our unique experience to support researchers who are interested in
exploring autonomous driving transportation services in various real-world
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Out-of-Distribution Generalization of Learned Dynamics by
  Learning Pseudometrics and Constraint Manifolds <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yating Lin, Glen Chou, Dmitry Berenson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method for improving the prediction accuracy of learned robot
dynamics models on out-of-distribution (OOD) states. We achieve this by
leveraging two key sources of structure often present in robot dynamics: 1)
sparsity, i.e., some components of the state may not affect the dynamics, and
2) physical limits on the set of possible motions, in the form of nonholonomic
constraints. Crucially, we do not assume this structure is known a priori, and
instead learn it from data. We use contrastive learning to obtain a distance
pseudometric that uncovers the sparsity pattern in the dynamics, and use it to
reduce the input space when learning the dynamics. We then learn the unknown
constraint manifold by approximating the normal space of possible motions from
the data, which we use to train a Gaussian process (GP) representation of the
constraint manifold. We evaluate our approach on a physical differential-drive
robot and a simulated quadrotor, showing improved prediction accuracy on OOD
data relative to baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ICRA 2024, 6 pages + references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LingoQA: Video Question Answering for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ana-Maria Marcu, Long Chen, Jan Hünermann, Alice Karnsund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, Elahe Arani, Oleg Sinavski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving has long faced a challenge with public acceptance due to
the lack of explainability in the decision-making process. Video
question-answering (QA) in natural language provides the opportunity for
bridging this gap. Nonetheless, evaluating the performance of Video QA models
has proved particularly tough due to the absence of comprehensive benchmarks.
To fill this gap, we introduce LingoQA, a benchmark specifically for autonomous
driving Video QA. The LingoQA trainable metric demonstrates a 0.95 Spearman
correlation coefficient with human evaluations. We introduce a Video QA dataset
of central London consisting of 419k samples that we release with the paper. We
establish a baseline vision-language model and run extensive ablation studies
to understand its performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Benchmark and dataset are available at
  https://github.com/wayveai/LingoQA/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-19T00:00:00Z">2024-03-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">26</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaDRE: <span class="highlight-title">Control</span>lable and Diverse Generation of Safety-Critical Driving
  Scenarios using Real-World Trajectories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peide Huang, Wenhao Ding, Jonathan Francis, Bingqing Chen, Ding Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation is an indispensable tool in the development and testing of
autonomous vehicles (AVs), offering an efficient and safe alternative to road
testing by allowing the exploration of a wide range of scenarios. Despite its
advantages, a significant challenge within simulation-based testing is the
generation of safety-critical scenarios, which are essential to ensure that AVs
can handle rare but potentially fatal situations. This paper addresses this
challenge by introducing a novel generative framework, CaDRE, which is
specifically designed for generating diverse and controllable safety-critical
scenarios using real-world trajectories. Our approach optimizes for both the
quality and diversity of scenarios by employing a unique formulation and
algorithm that integrates real-world data, domain knowledge, and black-box
optimization techniques. We validate the effectiveness of our framework through
extensive testing in three representative types of traffic scenarios. The
results demonstrate superior performance in generating diverse and high-quality
scenarios with greater sample efficiency than existing reinforcement learning
and sampling-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robots That Know When They Need Help: Affordance-Based
  Uncertainty for Large Language Model Planners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James F. Mullen Jr., Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) showcase many desirable traits for intelligent
and helpful robots. However, they are also known to hallucinate predictions.
This issue is exacerbated in consumer robotics where LLM hallucinations may
result in robots confidently executing plans that are contrary to user goals,
relying more frequently on human assistance, or preventing the robot from
asking for help at all. In this work, we present LAP, a novel approach for
utilizing off-the-shelf LLM's, alongside scene and object Affordances, in
robotic Planners that minimize harmful hallucinations and know when to ask for
help. Our key finding is that calculating and leveraging a scene affordance
score, a measure of whether a given action is possible in the provided scene,
helps to mitigate hallucinations in LLM predictions and better align the LLM's
confidence measure with the probability of success. We specifically propose and
test three different affordance scores, which can be used independently or in
tandem to improve performance across different use cases. The most successful
of these individual scores involves prompting an LLM to determine if a given
action is possible and safe in the given scene and uses the LLM's response to
compute the score. Through experiments in both simulation and the real world,
on tasks with a variety of ambiguities, we show that LAP significantly
increases success rate and decreases the amount of human intervention required
relative to prior art. For example, in our real-world testing paradigm, LAP
decreases the human help rate of previous methods by over 33% at a success rate
of 70%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reflectivity Is All You Need!: Advancing LiDAR Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kasi Viswanath, Peng Jiang, Srikanth Saripalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR semantic segmentation frameworks predominantly leverage geometry-based
features to differentiate objects within a scan. While these methods excel in
scenarios with clear boundaries and distinct shapes, their performance declines
in environments where boundaries are blurred, particularly in off-road
contexts. To address this, recent strides in 3D segmentation algorithms have
focused on harnessing raw LiDAR intensity measurements to improve prediction
accuracy. Despite these efforts, current learning-based models struggle to
correlate the intricate connections between raw intensity and factors such as
distance, incidence angle, material reflectivity, and atmospheric conditions.
Building upon our prior work, this paper delves into the advantages of
employing calibrated intensity (also referred to as reflectivity) within
learning-based LiDAR semantic segmentation frameworks. We initially establish
that incorporating reflectivity as an input enhances the existing LiDAR
semantic segmentation model. Furthermore, we present findings that enable the
model to learn to calibrate intensity can boost its performance. Through
extensive experimentation on the off-road dataset Rellis-3D, we demonstrate
notable improvements. Specifically, converting intensity to reflectivity
results in a 4% increase in mean Intersection over Union (mIoU) when compared
to using raw intensity in Off-road scenarios. Additionally, we also investigate
the possible benefits of using calibrated intensity in semantic segmentation in
urban environments (SemanticKITTI) and cross-sensor domain adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User-customizable Shared <span class="highlight-title">Control</span> for Fine Teleoperation via Virtual
  Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Luo, Mark Zolotas, Drake Moore, Taskin Padir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shared control can ease and enhance a human operator's ability to teleoperate
robots, particularly for intricate tasks demanding fine control over multiple
degrees of freedom. However, the arbitration process dictating how much
autonomous assistance to administer in shared control can confuse novice
operators and impede their understanding of the robot's behavior. To overcome
these adverse side-effects, we propose a novel formulation of shared control
that enables operators to tailor the arbitration to their unique capabilities
and preferences. Unlike prior approaches to customizable shared control where
users could indirectly modify the latent parameters of the arbitration function
by issuing a feedback command, we instead make these parameters observable and
directly editable via a virtual reality (VR) interface. We present our
user-customizable shared control method for a teleoperation task in SE(3),
known as the buzz wire game. A user study is conducted with participants
teleoperating a robotic arm in VR to complete the game. The experiment spanned
two weeks per subject to investigate longitudinal trends. Our findings reveal
that users allowed to interactively tune the arbitration parameters across
trials generalize well to adaptations in the task, exhibiting improvements in
precision and fluency over direct teleoperation and conventional shared
control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Designing Consistent Covariance Recovery from a Deep Learning Visual
  Odometry Engine <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jagatpreet Singh Nir, Dennis Giaya, Hanumant Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques have significantly advanced in providing accurate
visual odometry solutions by leveraging large datasets. However, generating
uncertainty estimates for these methods remains a challenge. Traditional sensor
fusion approaches in a Bayesian framework are well-established, but deep
learning techniques with millions of parameters lack efficient methods for
uncertainty estimation.
  This paper addresses the issue of uncertainty estimation for pre-trained
deep-learning models in monocular visual odometry. We propose formulating a
factor graph on an implicit layer of the deep learning network to recover
relative covariance estimates, which allows us to determine the covariance of
the Visual Odometry (VO) solution. We showcase the consistency of the deep
learning engine's covariance approximation with an empirical analysis of the
covariance model on the EUROC datasets to demonstrate the correctness of our
formulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Learning for Fast Adaptation in Intent Inferral on a Robotic Hand
  Orthosis for Stroke 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Leandro La Rotta, Jingxi Xu, Ava Chen, Lauren Winterbottom, Wenxi Chen, Dawn Nilsen, Joel Stein, Matei Ciocarlie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MetaEMG, a meta-learning approach for fast adaptation in intent
inferral on a robotic hand orthosis for stroke. One key challenge in machine
learning for assistive and rehabilitative robotics with disabled-bodied
subjects is the difficulty of collecting labeled training data. Muscle tone and
spasticity often vary significantly among stroke subjects, and hand function
can even change across different use sessions of the device for the same
subject. We investigate the use of meta-learning to mitigate the burden of data
collection needed to adapt high-capacity neural networks to a new session or
subject. Our experiments on real clinical data collected from five stroke
subjects show that MetaEMG can improve the intent inferral accuracy with a
small session- or subject-specific dataset and very few fine-tuning epochs. To
the best of our knowledge, we are the first to formulate intent inferral on
stroke subjects as a meta-learning problem and demonstrate fast adaptation to a
new session or subject for controlling a robotic hand orthosis with EMG
signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Robot-Environment Self-Calibration via Compliant Exploratory
  Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Podshara Chanrungmaneekul, Kejia Ren, Joshua T. Grace, Aaron M. Dollar, Kaiyu Hang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calibrating robots into their workspaces is crucial for manipulation tasks.
Existing calibration techniques often rely on sensors external to the robot
(cameras, laser scanners, etc.) or specialized tools. This reliance complicates
the calibration process and increases the costs and time requirements.
Furthermore, the associated setup and measurement procedures require
significant human intervention, which makes them more challenging to operate.
Using the built-in force-torque sensors, which are nowadays a default component
in collaborative robots, this work proposes a self-calibration framework where
robot-environmental spatial relations are automatically estimated through
compliant exploratory actions by the robot itself. The self-calibration
approach converges, verifies its own accuracy, and terminates upon completion,
autonomously purely through interactive exploration of the environment's
geometries. Extensive experiments validate the effectiveness of our
self-calibration approach in accurately establishing the robot-environment
spatial relationships without the need for additional sensing equipment or any
human intervention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wearable Roller Rings to Enable Robot Dexterous In-Hand Manipulation
  through Active Surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hayden Webb, Podshara Chanrungmaneekul, Shenli Yuan, Kaiyu Hang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-hand manipulation is a crucial ability for reorienting and repositioning
objects within grasps. The main challenges are not only the complexity in the
computational models, but also the risks of grasp instability caused by active
finger motions, such as rolling, sliding, breaking, and remaking contacts.
Based on the idea of manipulation without lifting a finger, this paper presents
the development of Roller Rings (RR), a modular robotic attachment with active
surfaces that is wearable by both robot and human hands. By installing and
angling the RRs on grasping systems, such that their spatial motions are not
co-linear, we derive a general differential motion model for the object
actuated by the active surfaces. Our motion model shows that complete in-hand
manipulation skill sets can be provided by as few as only 2 RRs through
non-holonomic object motions, while more RRs can enable enhanced manipulation
dexterity with fewer motion constraints. Through extensive experiments, we wear
RRs on both a robot hand and a human hand to evaluate their manipulation
capabilities, and show that the RRs can be employed to manipulate arbitrary
object shapes to provide dexterous in-hand manipulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Better Call SAL: Towards Learning to Segment Anything in Lidar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aljoša Ošep, Tim Meinhardt, Francesco Ferroni, Neehar Peri, Deva Ramanan, Laura Leal-Taixé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose $\texttt{SAL}$ ($\texttt{S}$egment $\texttt{A}$nything in
$\texttt{L}$idar) method consisting of a text-promptable zero-shot model for
segmenting and classifying any object in Lidar, and a pseudo-labeling engine
that facilitates model training without manual supervision. While the
established paradigm for $\textit{Lidar Panoptic Segmentation}$ (LPS) relies on
manual supervision for a handful of object classes defined a priori, we utilize
2D vision foundation models to generate 3D supervision "for free". Our
pseudo-labels consist of instance masks and corresponding CLIP tokens, which we
lift to Lidar using calibrated multi-modal data. By training our model on these
labels, we distill the 2D foundation models into our Lidar $\texttt{SAL}$
model. Even without manual labels, our model achieves $91\%$ in terms of
class-agnostic segmentation and $44\%$ in terms of zero-shot LPS of the fully
supervised state-of-the-art. Furthermore, we outperform several baselines that
do not distill but only lift image features to 3D. More importantly, we
demonstrate that $\texttt{SAL}$ supports arbitrary class prompts, can be easily
extended to new datasets, and shows significant potential to improve with
increasing amounts of self-labeled data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cooperative Modular Manipulation with Numerous Cable-Driven Robots for
  Assistive Construction and Gap Crossing <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Murphy, Joao C. V. Soares, Justin K. Yim, Dustin Nottage, Ahmet Soylemezoglu, Joao Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soldiers in the field often need to cross negative obstacles, such as rivers
or canyons, to reach goals or safety. Military gap crossing involves on-site
temporary bridges construction. However, this procedure is conducted with
dangerous, time and labor intensive operations, and specialized machinery. We
envision a scalable robotic solution inspired by advancements in
force-controlled and Cable Driven Parallel Robots (CDPRs); this solution can
address the challenges inherent in this transportation problem, achieving fast,
efficient, and safe deployment and field operations. We introduce the embodied
vision in Co3MaNDR, a solution to the military gap crossing problem, a
distributed robot consisting of several modules simultaneously pulling on a
central payload, controlling the cables' tensions to achieve complex
objectives, such as precise trajectory tracking or force amplification.
Hardware experiments demonstrate teleoperation of a payload, trajectory
following, and the sensing and amplification of operators' applied physical
forces during slow operations. An operator was shown to manipulate a 27.2 kg
(60 lb) payload with an average force utilization of 14.5\% of its weight.
Results indicate that the system can be scaled up to heavier payloads without
compromising performance or introducing superfluous complexity. This research
lays a foundation to expand CDPR technology to uncoordinated and unstable
mobile platforms in unknown environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures. Submit to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Network-based Multi-agent <span class="highlight-title">Reinforcement Learning</span> for
  Resilient Distributed Coordination of Multi-Robot Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Goeckner, Yueyuan Sui, Nicolas Martinet, Xinliang Li, Qi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing multi-agent coordination techniques are often fragile and vulnerable
to anomalies such as agent attrition and communication disturbances, which are
quite common in the real-world deployment of systems like field robotics. To
better prepare these systems for the real world, we present a graph neural
network (GNN)-based multi-agent reinforcement learning (MARL) method for
resilient distributed coordination of a multi-robot system. Our method,
Multi-Agent Graph Embedding-based Coordination (MAGEC), is trained using
multi-agent proximal policy optimization (PPO) and enables distributed
coordination around global objectives under agent attrition, partial
observability, and limited or disturbed communications. We use a multi-robot
patrolling scenario to demonstrate our MAGEC method in a ROS 2-based simulator
and then compare its performance with prior coordination approaches. Results
demonstrate that MAGEC outperforms existing methods in several experiments
involving agent attrition and communication disturbance, and provides
competitive results in scenarios without such anomalies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Twin-Driven <span class="highlight-title">Reinforcement Learning</span> for Obstacle Avoidance in
  Robot Manipulators: A Self-Improving Online Training Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhu Sun, Mien Van, Stephen McIlvanna, Nguyen Minh Nhat, Kabirat Olayemi, Jack Close, Seán McLoone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution and growing automation of collaborative robots introduce more
complexity and unpredictability to systems, highlighting the crucial need for
robot's adaptability and flexibility to address the increasing complexities of
their environment. In typical industrial production scenarios, robots are often
required to be re-programmed when facing a more demanding task or even a few
changes in workspace conditions. To increase productivity, efficiency and
reduce human effort in the design process, this paper explores the potential of
using digital twin combined with Reinforcement Learning (RL) to enable robots
to generate self-improving collision-free trajectories in real time. The
digital twin, acting as a virtual counterpart of the physical system, serves as
a 'forward run' for monitoring, controlling, and optimizing the physical system
in a safe and cost-effective manner. The physical system sends data to
synchronize the digital system through the video feeds from cameras, which
allows the virtual robot to update its observation and policy based on real
scenarios. The bidirectional communication between digital and physical systems
provides a promising platform for hardware-in-the-loop RL training through
trial and error until the robot successfully adapts to its new environment. The
proposed online training framework is demonstrated on the Unfactory Xarm5
collaborative robot, where the robot end-effector aims to reach the target
position while avoiding obstacles. The experiment suggest that proposed
framework is capable of performing policy online training, and that there
remains significant room for improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subgoal Diffuser: Coarse-to-fine Subgoal Generation to Guide Model
  Predictive <span class="highlight-title">Control</span> for Robot Manipulation <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Huang, Yating Lin, Fan Yang, Dmitry Berenson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulation of articulated and deformable objects can be difficult due to
their compliant and under-actuated nature. Unexpected disturbances can cause
the object to deviate from a predicted state, making it necessary to use
Model-Predictive Control (MPC) methods to plan motion. However, these methods
need a short planning horizon to be practical. Thus, MPC is ill-suited for
long-horizon manipulation tasks due to local minima. In this paper, we present
a diffusion-based method that guides an MPC method to accomplish long-horizon
manipulation tasks by dynamically specifying sequences of subgoals for the MPC
to follow. Our method, called Subgoal Diffuser, generates subgoals in a
coarse-to-fine manner, producing sparse subgoals when the task is easily
accomplished by MPC and more dense subgoals when the MPC method needs more
guidance. The density of subgoals is determined dynamically based on a learned
estimate of reachability, and subgoals are distributed to focus on challenging
parts of the task. We evaluate our method on two robot manipulation tasks and
find it improves the planning performance of an MPC method, and also
outperforms prior diffusion-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Current-Based Impedance <span class="highlight-title">Control</span> for Interacting with Mobile Manipulators <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jelmer de Wolde, Luzia Knoedler, Gianluca Garofalo, Javier Alonso-Mora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As robots shift from industrial to human-centered spaces, adopting mobile
manipulators, which expand workspace capabilities, becomes crucial. In these
settings, seamless interaction with humans necessitates compliant control. Two
common methods for safe interaction, admittance, and impedance control, require
force or torque sensors, often absent in lower-cost or lightweight robots. This
paper presents an adaption of impedance control that can be used on
current-controlled robots without the use of force or torque sensors and its
application for compliant control of a mobile manipulator. A calibration method
is designed that enables estimation of the actuators' current/torque ratios and
frictions, used by the adapted impedance controller, and that can handle model
errors. The calibration method and the performance of the designed controller
are experimentally validated using the Kinova GEN3 Lite arm. Results show that
the calibration method is consistent and that the designed controller for the
arm is compliant while also being able to track targets with five-millimeter
precision when no interaction is present. Additionally, this paper presents two
operational modes for interacting with the mobile manipulator: one for guiding
the robot around the workspace through interacting with the arm and another for
executing a tracking task, both maintaining compliance to external forces.
These operational modes were tested in real-world experiments, affirming their
practical applicability and effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 13 figures, under review for IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WHAC: World-grounded Humans and Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanqi Yin, Zhongang Cai, Ruisi Wang, Fanzhou Wang, Chen Wei, Haiyi Mei, Weiye Xiao, Zhitao Yang, Qingping Sun, Atsushi Yamashita, Ziwei Liu, Lei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating human and camera trajectories with accurate scale in the world
coordinate system from a monocular video is a highly desirable yet challenging
and ill-posed problem. In this study, we aim to recover expressive parametric
human models (i.e., SMPL-X) and corresponding camera poses jointly, by
leveraging the synergy between three critical players: the world, the human,
and the camera. Our approach is founded on two key observations. Firstly,
camera-frame SMPL-X estimation methods readily recover absolute human depth.
Secondly, human motions inherently provide absolute spatial cues. By
integrating these insights, we introduce a novel framework, referred to as
WHAC, to facilitate world-grounded expressive human pose and shape estimation
(EHPS) alongside camera pose estimation, without relying on traditional
optimization techniques. Additionally, we present a new synthetic dataset,
WHAC-A-Mole, which includes accurately annotated humans and cameras, and
features diverse interactive human motions as well as realistic camera
trajectories. Extensive experiments on both standard and newly established
benchmarks highlight the superiority and efficacy of our framework. We will
make the code and dataset publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://wqyin.github.io/projects/WHAC/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAPTR: Tracking Any Point with Transformers as Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a simple and strong framework for Tracking Any
Point with TRansformers (TAPTR). Based on the observation that point tracking
bears a great resemblance to object detection and tracking, we borrow designs
from DETR-like algorithms to address the task of TAP. In the proposed
framework, in each video frame, each tracking point is represented as a point
query, which consists of a positional part and a content part. As in DETR, each
query (its position and content feature) is naturally updated layer by layer.
Its visibility is predicted by its updated content feature. Queries belonging
to the same tracking point can exchange information through self-attention
along the temporal dimension. As all such operations are well-designed in
DETR-like algorithms, the model is conceptually very simple. We also adopt some
useful designs such as cost volume from optical flow models and develop simple
designs to provide long temporal information while mitigating the feature
drifting issue. Our framework demonstrates strong performance with
state-of-the-art performance on various TAP datasets with faster inference
speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DROID: A Large-Scale In-The-Wild Robot Manipulation <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag R Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Daphne Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Donovon Jackson, Charlotte Le, Yunshuang Li, Kevin Lin, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail O'Neill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph J Lim, Jitendra Malik, Roberto Martín-Martín, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, <span class="highlight-author">Sergey Levine</span>, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The creation of large, diverse, high-quality robot manipulation datasets is
an important stepping stone on the path toward more capable and robust robotic
manipulation policies. However, creating such datasets is challenging:
collecting robot manipulation data in diverse environments poses logistical and
safety challenges and requires substantial investments in hardware and human
labour. As a result, even the most general robot manipulation policies today
are mostly trained on data collected in a small number of environments with
limited scene and task diversity. In this work, we introduce DROID (Distributed
Robot Interaction Dataset), a diverse robot manipulation dataset with 76k
demonstration trajectories or 350 hours of interaction data, collected across
564 scenes and 84 tasks by 50 data collectors in North America, Asia, and
Europe over the course of 12 months. We demonstrate that training with DROID
leads to policies with higher performance and improved generalization ability.
We open source the full dataset, policy learning code, and a detailed guide for
reproducing our robot hardware setup.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://droid-dataset.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vid2Robot: End-to-end Video-conditioned Policy Learning with
  Cross-Attention Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vidhi Jain, Maria Attarian, Nikhil J Joshi, Ayzaan Wahid, Danny Driess, Quan Vuong, Pannag R Sanketi, Pierre Sermanet, Stefan Welker, Christine Chan, Igor Gilitschenski, Yonatan Bisk, Debidatta Dwibedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large-scale robotic systems typically rely on textual instructions for
tasks, this work explores a different approach: can robots infer the task
directly from observing humans? This shift necessitates the robot's ability to
decode human intent and translate it into executable actions within its
physical constraints and environment. We introduce Vid2Robot, a novel
end-to-end video-based learning framework for robots. Given a video
demonstration of a manipulation task and current visual observations, Vid2Robot
directly produces robot actions. This is achieved through a unified
representation model trained on a large dataset of human video and robot
trajectory. The model leverages cross-attention mechanisms to fuse prompt video
features to the robot's current state and generate appropriate actions that
mimic the observed task. To further improve policy performance, we propose
auxiliary contrastive losses that enhance the alignment between human and robot
video representations. We evaluate Vid2Robot on real-world robots,
demonstrating a 20% improvement in performance compared to other
video-conditioned policies when using human demonstration videos. Additionally,
our model exhibits emergent capabilities, such as successfully transferring
observed motions from one object to another, and long-horizon composition, thus
showcasing its potential for real-world applications. Project website:
vid2robot.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Robot learning: Imitation Learning, Robot Perception, Sensing &
  Vision, Grasping & Manipulation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Layering in Room Segmentation via LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taehyeon Kim, Byung-Cheol Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Semantic Layering in Room Segmentation via LLMs
(SeLRoS), an advanced method for semantic room segmentation by integrating
Large Language Models (LLMs) with traditional 2D map-based segmentation. Unlike
previous approaches that solely focus on the geometric segmentation of indoor
environments, our work enriches segmented maps with semantic data, including
object identification and spatial relationships, to enhance robotic navigation.
By leveraging LLMs, we provide a novel framework that interprets and organizes
complex information about each segmented area, thereby improving the accuracy
and contextual relevance of room segmentation. Furthermore, SeLRoS overcomes
the limitations of existing algorithms by using a semantic evaluation method to
accurately distinguish true room divisions from those erroneously generated by
furniture and segmentation inaccuracies. The effectiveness of SeLRoS is
verified through its application across 30 different 3D environments. Source
code and experiment videos for this work are available at:
https://sites.google.com/view/selros.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Yell At Your Robot: Improving On-the-Fly from Language Corrections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, <span class="highlight-author">Sergey Levine</span>, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical policies that combine language and low-level control have been
shown to perform impressively long-horizon robotic tasks, by leveraging either
zero-shot high-level planners like pretrained language and vision-language
models (LLMs/VLMs) or models trained on annotated robotic demonstrations.
However, for complex and dexterous skills, attaining high success rates on
long-horizon tasks still represents a major challenge -- the longer the task
is, the more likely it is that some stage will fail. Can humans help the robot
to continuously improve its long-horizon task performance through intuitive and
natural feedback? In this paper, we make the following observation: high-level
policies that index into sufficiently rich and expressive low-level
language-conditioned skills can be readily supervised with human feedback in
the form of language corrections. We show that even fine-grained corrections,
such as small movements ("move a bit to the left"), can be effectively
incorporated into high-level policies, and that such corrections can be readily
obtained from humans observing the robot and making occasional suggestions.
This framework enables robots not only to rapidly adapt to real-time language
feedback, but also incorporate this feedback into an iterative training scheme
that improves the high-level policy's ability to correct errors in both
low-level execution and high-level decision-making purely from verbal feedback.
Our evaluation on real hardware shows that this leads to significant
performance improvement in long-horizon, dexterous manipulation tasks without
the need for any additional teleoperation. Videos and code are available at
https://yay-robot.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://yay-robot.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OSDaR23: Open Sensor Data for Rail 2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rustam Tagiew, Martin Köppel, Karsten Schwalbe, Patrick Denzler, Philipp Neumaier, Tobias Klockau, Martin Boekhoff, Pavel Klasek, Roman Tilly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve a driverless train operation on mainline railways, actual and
potential obstacles for the train's driveway must be detected automatically by
appropriate sensor systems. Machine learning algorithms have proven to be
powerful tools for this task during the last years. However, these algorithms
require large amounts of high-quality annotated data containing
railway-specific objects as training data. Unfortunately, all of the publicly
available datasets that tackle this requirement are restricted in some way.
Therefore, this paper presents OSDaR23, a multi-sensor dataset of 45
subsequences acquired in Hamburg, Germany, in September 2021, that was created
to foster driverless train operation on mainline railways. The sensor setup
consists of multiple calibrated and synchronized infrared (IR) and visual (RGB)
cameras, lidars, a radar, and position and acceleration sensors mounted on the
front of a rail vehicle. In addition to the raw data, the dataset contains
204091 polyline, polygonal, rectangle, and cuboid annotations in total for 20
different object classes. It is the first publicly available multi-sensor
dataset annotated with a variety of object classes that are relevant for the
railway context. OSDaR23, available at data.fid-move.de/dataset/osdar23, can
also be used for tasks beyond collision prediction, which are listed in this
paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 11 images, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical <span class="highlight-title">Optimization</span>-based <span class="highlight-title">Control</span> for Whole-body Loco-manipulation
  of Heavy Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00112v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00112v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Rigo, Muqun Hu, Satyandra K. Gupta, Quan Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the field of legged robotics has seen growing interest in
enhancing the capabilities of these robots through the integration of
articulated robotic arms. However, achieving successful loco-manipulation,
especially involving interaction with heavy objects, is far from
straightforward, as object manipulation can introduce substantial disturbances
that impact the robot's locomotion. This paper presents a novel framework for
legged loco-manipulation that considers whole-body coordination through a
hierarchical optimization-based control framework. First, an online
manipulation planner computes the manipulation forces and manipulated object
task-based reference trajectory. Then, pose optimization aligns the robot's
trajectory with kinematic constraints. The resultant robot reference trajectory
is executed via a linear MPC controller incorporating the desired manipulation
forces into its prediction model. Our approach has been validated in simulation
and hardware experiments, highlighting the necessity of whole-body optimization
compared to the baseline locomotion MPC when interacting with heavy objects.
Experimental results with Unitree Aliengo, equipped with a custom-made robotic
arm, showcase its ability to lift and carry an 8kg payload and manipulate
doors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Agile <span class="highlight-title">Locomotion</span> and Adaptive Behaviors via RL-augmented <span class="highlight-title">MPC</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09442v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09442v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyu Chen, Quan Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of legged robots, adaptive behavior involves adaptive
balancing and adaptive swing foot reflection. While adaptive balancing
counteracts perturbations to the robot, adaptive swing foot reflection helps
the robot to navigate intricate terrains without foot entrapment. In this
paper, we manage to bring both aspects of adaptive behavior to quadruped
locomotion by combining RL and MPC while improving the robustness and agility
of blind legged locomotion. This integration leverages MPC's strength in
predictive capabilities and RL's adeptness in drawing from past experiences.
Unlike traditional locomotion controls that separate stance foot control and
swing foot trajectory, our innovative approach unifies them, addressing their
lack of synchronization. At the heart of our contribution is the synthesis of
stance foot control with swing foot reflection, improving agility and
robustness in locomotion with adaptive behavior. A hallmark of our approach is
robust blind stair climbing through swing foot reflection. Moreover, we
intentionally designed the learning module as a general plugin for different
robot platforms. We trained the policy and implemented our approach on the
Unitree A1 robot, achieving impressive results: a peak turn rate of 8.5 rad/s,
a peak running speed of 3 m/s, and steering at a speed of 2.5 m/s. Remarkably,
this framework also allows the robot to maintain stable locomotion while
bearing an unexpected load of 10 kg, or 83\% of its body mass. We further
demonstrate the generalizability and robustness of the same policy where it
realizes zero-shot transfer to different robot platforms like Go1 and AlienGo
robots for load carrying. Code is made available for the use of the research
community at https://github.com/DRCL-USC/RL_augmented_MPC.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Locomotion</span> Generation for a Rat Robot based on Environmental Changes via
  <span class="highlight-title">Reinforcement Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhui Shan, Yuhong Huang, Zhenshan Bing, Zitao Zhang, Xiangtong Yao, Kai Huang, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research focuses on developing reinforcement learning approaches for the
locomotion generation of small-size quadruped robots. The rat robot NeRmo is
employed as the experimental platform. Due to the constrained volume,
small-size quadruped robots typically possess fewer and weaker sensors,
resulting in difficulty in accurately perceiving and responding to
environmental changes. In this context, insufficient and imprecise feedback
data from sensors makes it difficult to generate adaptive locomotion based on
reinforcement learning. To overcome these challenges, this paper proposes a
novel reinforcement learning approach that focuses on extracting effective
perceptual information to enhance the environmental adaptability of small-size
quadruped robots. According to the frequency of a robot's gait stride, key
information of sensor data is analyzed utilizing sinusoidal functions derived
from Fourier transform results. Additionally, a multifunctional reward
mechanism is proposed to generate adaptive locomotion in different tasks.
Extensive simulations are conducted to assess the effectiveness of the proposed
reinforcement learning approach in generating rat robot locomotion in various
environments. The experiment results illustrate the capability of the proposed
approach to maintain stable locomotion of a rat robot across different
terrains, including ramps, stairs, and spiral stairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LANCAR: Leveraging Language for Context-Aware Robot <span class="highlight-title">Locomotion</span> in
  Unstructured Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chak Lam Shek, Xiyang Wu, Wesley A. Suttle, Carl Busart, Erin Zaroukian, Dinesh Manocha, Pratap Tokekar, Amrit Singh Bedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating robots through unstructured terrains is challenging, primarily due
to the dynamic environmental changes. While humans adeptly navigate such
terrains by using context from their observations, creating a similar
context-aware navigation system for robots is difficult. The essence of the
issue lies in the acquisition and interpretation of contextual information, a
task complicated by the inherent ambiguity of human language. In this work, we
introduce LANCAR, which addresses this issue by combining a context translator
with reinforcement learning (RL) agents for context-aware locomotion. LANCAR
allows robots to comprehend contextual information through Large Language
Models (LLMs) sourced from human observers and convert this information into
actionable contextual embeddings. These embeddings, combined with the robot's
sensor data, provide a complete input for the RL agent's policy network. We
provide an extensive evaluation of LANCAR under different levels of contextual
ambiguity and compare with alternative methods. The experimental results
showcase the superior generalizability and adaptability across different
terrains. Notably, LANCAR shows at least a 7.4% increase in episodic reward
over the best alternatives, highlighting its potential to enhance robotic
navigation in unstructured environments. More details and experiment videos
could be found in http://raaslab.org/projects/LLM_Context_Estimation/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLingua: Improving <span class="highlight-title">Reinforcement Learning</span> Sample Efficiency in Robotic
  Manipulations With Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangliang Chen, Yutian Lei, Shiyu Jin, Ying Zhang, Liangjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has demonstrated its capability in solving
various tasks but is notorious for its low sample efficiency. In this paper, we
propose RLingua, a framework that can leverage the internal knowledge of large
language models (LLMs) to reduce the sample complexity of RL in robotic
manipulations. To this end, we first present a method for extracting the prior
knowledge of LLMs by prompt engineering so that a preliminary rule-based robot
controller for a specific task can be generated in a user-friendly manner.
Despite being imperfect, the LLM-generated robot controller is utilized to
produce action samples during rollouts with a decaying probability, thereby
improving RL's sample efficiency. We employ TD3, the widely-used RL baseline
method, and modify the actor loss to regularize the policy learning towards the
LLM-generated controller. RLingua also provides a novel method of improving the
imperfect LLM-generated robot controllers by RL. We demonstrate that RLingua
can significantly reduce the sample complexity of TD3 in four robot tasks of
panda_gym and achieve high success rates in 12 sampled sparsely rewarded robot
tasks in RLBench, where the standard TD3 fails. Additionally, We validated
RLingua's effectiveness in real-world robot experiments through Sim2Real,
demonstrating that the learned policies are effectively transferable to real
robot tasks. Further details about our work are available at our project
website https://rlingua.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-03-24T08:15:33.408098512Z">
            2024-03-24 08:15:33 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
